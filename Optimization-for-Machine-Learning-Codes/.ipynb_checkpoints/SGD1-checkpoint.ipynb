{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: Stochastic Gradient Descent SOLUTIONS\n",
    "\n",
    "The objective of this lab session is to implement and test:\n",
    "- Stochastic gradient descent with constant stepsizes\n",
    "- Stochastic gradient descent with shrinking stepsizes\n",
    "- Stochastic gradient descent with sampling with/without replacement\n",
    "- Stochastic gradient descent with averaging \n",
    "\n",
    "and compare your implementation with gradient descent.\n",
    "\n",
    "Throughout the notebook you will find commented boxes like this one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ###   \n",
    "# please implement blabla\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boxes need to be replaced by code as explained in the boxes.\n",
    "Solutions will online tomorrow. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, jit, jitclass  # A just in time compiler to speed things up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "## 1. Loss functions, gradients and step-sizes\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n \\ell(x_i^\\top w, b_i) + \\frac \\lambda 2 \\|w\\|_2^2\n",
    "$$\n",
    "where\n",
    "- $\\ell(z, b) = \\frac 12 (b - z)^2$ (least-squares regression)\n",
    "- $\\ell(z, b) = \\log(1 + \\exp(-bz))$ (logistic regression).\n",
    "\n",
    "We write it as a a minimization problem of the form\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n f_i(w)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(w) = \\ell(x_i^\\top w, y_i) + \\frac \\lambda 2 \\|w\\|_2^2.\n",
    "$$\n",
    "\n",
    "For both cases, the gradients are\n",
    "$$\n",
    "\\nabla f_i(w) = (x_i^\\top w - y_i) x_i + \\lambda w\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla f_i(w) = - \\frac{y_i}{1 + \\exp(y_i x_i^\\top w)} x_i + \\lambda w.\n",
    "$$\n",
    "\n",
    "Denote by $L$ the Lipschitz constant of $f$ and $X = [x_1, \\ldots, x_n].$\n",
    "One can see easily that for linear regression\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X \\mathbf X^\\top \\|_{2}}{n} + \\lambda \n",
    "$$\n",
    "while for logistic regression it is\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X \\mathbf X^\\top \\|_{2}}{4 n} + \\lambda \n",
    "$$\n",
    "For full-gradient methods, the theoretical step-size is $1 / L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce a class that will be used for the solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "class LinReg(object):\n",
    "    \"\"\"A class for the least-squares regression with\n",
    "    Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lbda):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        self.lbda = lbda  \n",
    "      \n",
    "    def grad(self, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f\n",
    "        #############\n",
    "        return np.zeros(np.size(w))\n",
    "    \n",
    "    def f_i(self, i, w):\n",
    "        return norm(self.X[i].dot(w) - self.y[i]) ** 2 / (2.) + self.lbda * norm(w) ** 2 / 2.      \n",
    "    \n",
    "    def f(self, w):\n",
    "        return norm(self.X.dot(w) - self.y) ** 2 / (2. * self.n) + self.lbda * norm(w) ** 2 / 2.\n",
    "# (1/n)X_i (X_i^Tw-y_i) + lbda*w    \n",
    "    def grad_i(self, i, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f_i\n",
    "        #############\n",
    "        return np.zeros(np.size(w))\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the smoothness constant (Lipschitz constant of the gradient)\"\"\"\n",
    "        L = norm(self.X, ord=2) ** 2 / self.n + self.lbda\n",
    "        return L\n",
    "    \n",
    "    def L_max_constant(self):\n",
    "        \"\"\"Return the L_max constant \"\"\"\n",
    "        L_max = np.max(np.sum(self.X ** 2, axis=1)) + self.lbda\n",
    "        return L_max \n",
    "    \n",
    "    def mu_constant(self):\n",
    "        \"\"\"Return the strong convexity constant\"\"\"\n",
    "        mu =  min(abs(la.eigvals(np.dot(self.X.T,self.X)))) / self.n + self.lbda\n",
    "        return mu     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \"\"\"A class for the logistic regression with L2 penalization\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, lbda):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n, self.d = X.shape\n",
    "        self.lbda = lbda\n",
    " \n",
    "    def grad(self, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f\n",
    "        #############\n",
    "        return np.zeros(np.size(w))\n",
    "\n",
    "    def f_i(self,i, w):\n",
    "        bAx_i = self.y[i] * np.dot(self.X[i], w)\n",
    "        return np.log(1. + np.exp(- bAx_i)) + self.lbda * norm(w) ** 2 / 2.\n",
    "    \n",
    "    def f(self, w):\n",
    "        bAx = self.y * self.X.dot(w)\n",
    "        return np.mean(np.log(1. + np.exp(- bAx))) + self.lbda * norm(w) ** 2 / 2.\n",
    "  \n",
    "    def grad_i(self, i, w):\n",
    "        ### TODO ###   \n",
    "        # calculate the gradient of f_i\n",
    "        #############\n",
    "        return np.zeros(np.size(w))\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the smoothness constant (Lipschitz constant of the gradient)\"\"\"\n",
    "        L = norm(self.X, ord=2) ** 2  / (4. * self.n) + self.lbda\n",
    "        return L\n",
    "    def L_max_constant(self):\n",
    "        \"\"\"Return the L_max constant \"\"\"\n",
    "        L_max = np.max(np.sum(self.X ** 2, axis=1))/4 + self.lbda\n",
    "        return L_max \n",
    "    \n",
    "    def mu_constant(self):\n",
    "        \"\"\"Return the strong convexity constant\"\"\"\n",
    "        mu =  self.lbda\n",
    "        return mu    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 2. Generate a dataset\n",
    "\n",
    "We generate datasets for the least-squares and the logistic cases. First we define a function for the least-squares case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "    \n",
    "def simu_linreg(w, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the least-squares problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    d = w.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    X = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    y = X.dot(w) + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simu_logreg(w, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the logistic regression problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    X, y = simu_linreg(w, n, std=1., corr=0.5)\n",
    "    return X, np.sign(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 50\n",
    "n = 1000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "w_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "# X, y = simu_linreg(w_model_truth, n, std=1., corr=0.1)\n",
    "X, y = simu_logreg(w_model_truth, n, std=1., corr=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choice of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbda = 1. / n ** (0.5)\n",
    "model = LinReg(X, y, lbda)\n",
    "#model = LogReg(X, y, lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(w_model_truth); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the gradient grad_i and the numerical gradient of f_i agree\n",
    "grad_error = []\n",
    "for i in range(n):\n",
    "    ind = np.random.choice(n,1)\n",
    "    w =  np.random.randn(d)\n",
    "    vec =  np.random.randn(d)\n",
    "    eps = pow(10.0, -7.0)\n",
    "    model.f_i(ind[0],w)\n",
    "    grad_error.append((model.f_i( ind[0], w+eps*vec) - model.f_i( ind[0], w))/eps - np.dot(model.grad_i(ind[0],w),vec))\n",
    "plt.stem(grad_error); \n",
    "print(np.mean(grad_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "modellog = LogReg(X, y, lbda)\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(modellog.f, modellog.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellin = LinReg(X, y, lbda)\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(modellin.f, modellin.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a highly accurate solution using LBFGS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "w_init = np.zeros(d)\n",
    "w_min, obj_min, _ = fmin_l_bfgs_b(model.f, w_init, model.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "\n",
    "print(obj_min)\n",
    "print(norm(model.grad(w_min)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='constant'></a> \n",
    "\n",
    "## 3. Implementing Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "We recall that an iteration of SGD writes\n",
    "\n",
    "\n",
    "\n",
    "**for** $t = 1, \\ldots, T$ \n",
    "  \n",
    "$\\qquad$ Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "   \n",
    "$\\qquad \\displaystyle\n",
    "w^{t+1} \\gets w^t - \\gamma^t \\nabla f_i(w^t)\n",
    "$\n",
    "  \n",
    "**end for**\n",
    "\n",
    "\n",
    "\n",
    "Complete the code below. The inputs are\n",
    "- n_iter: The number of iterations\n",
    "- indices: an np.array of indices of length n_iter. The indices[t]  is the index of stochastic gradient that will be used on the t-th iteration. \n",
    "- steps: an np.array of positive floats of length n_iter. The steps[t] is the stepsize used on the kth iteration. Typically decreasing stepsizes are used.\n",
    "\n",
    "- averaging_on: is a boolean which indicates if the output should be the average of the iterates.\n",
    "\n",
    "The outputs are:\n",
    "- x_output: The final x vector found by the algorithm or the average  $ \\bar{w} = \\frac{1}{T-t}\\sum_{i=t}^T w^t$ if averaging is on\n",
    "- objectives: A ndarray containing the sequence function values calculated during the iterations of the algorithm \n",
    "- errors: If w_min is not empty, errors is a ndarray containing the sequence of errors || x - w_min || of the algorithm. Otherwise errors should be empty.\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SGD solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w0, model, indices, steps, w_min, n_iter=100, averaging_on=False ,verbose=True, start_averaging = 0):\n",
    "    \"\"\"Stochastic gradient descent algorithm\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    # average x\n",
    "    w_average = w0.copy()\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1.0\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    if np.any(w_min):\n",
    "        err = norm(w - w_min) / norm(w_min)\n",
    "        errors.append(err)\n",
    "    # Current objective\n",
    "    obj = model.f(w) \n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching SGD solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for t in range(n_iter):\n",
    "        ### TODO ###   \n",
    "        ####################################\n",
    "        # Compute the next iterate\n",
    "        #  w[:] =........\n",
    "        ####################################\n",
    "        ####################################\n",
    "        # Compute the average iterate \n",
    "        # w_average[:]  = ...w_average + .....\n",
    "        ####################################\n",
    "        if averaging_on:\n",
    "            w_test = w_average.copy()\n",
    "        else:\n",
    "            w_test = w.copy()\n",
    "        obj = model.f(w_test) \n",
    "        if np.any(w_min):\n",
    "            err = norm(w_test - w_min) / norm(w_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % n_samples == 0 and verbose:\n",
    "            if(sum(w_min)):\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "            else:\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8)]))\n",
    "    if averaging_on:\n",
    "        w_output = w_average.copy()\n",
    "    else:\n",
    "        w_output = w.copy()    \n",
    "    return w_output, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup number of iterations\n",
    "datapasses = 30  # number of sweeps through all the data. This means that there will datapasses*n stochastic gradient updates\n",
    "n_iter = int(datapasses * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with constant step with replacement\n",
    "\n",
    "Implement constant stepsizes with steps[t] $= 1/(2L_{\\max})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lmax = model.L_max_constant(); # Need this constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############TODO######################\n",
    "# Execute SGD with a constant stepsize. Please name the output as\n",
    "# w_sgdcr, obj_sgdcr, err_sgdcr = sgd(...?....)\n",
    "##############END TODO######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with $C/(t+1)$ stepsizes with replacement\n",
    "\n",
    "Try the following decreasing stepsizes steps[t] $= 1/(2L_{\\max}(t+1))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with a shrinking stepsize steps[t] = 1/(2L_{\\max}(t+1)). Please name the output as\n",
    "# w_sgdsr, obj_sgdsr, err_sgdsr = sgd(.....?.....)\n",
    "##############END TODO######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdcr - obj_min, label=\"SGD const\", lw=2)\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Error of objective\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdcr , label=\"SGD const\", lw=2)\n",
    "plt.semilogy(err_sgdsr , label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Compare the solution you obtain for SGD with constant stepsizes and SGD with shrinking stepsizes. \n",
    "- Which one is faster in the beginning? Which reaches the \"best\" solution?\n",
    "- What happens when is you use sampling without replacement instead? Hint: Do only one datapass, it's annoying to adapt this implementation for more than one datapass when sampling without replacement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with switch to  shrinking stepsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = model.mu_constant();\n",
    "Kappa = Lmax/mu;\n",
    "tstar = 4*int(np.ceil(Kappa));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SGD with the following switching stepsizes given in the lecture\n",
    "\n",
    "$\\gamma^t= \n",
    "\\begin{cases}\n",
    "\\displaystyle \\tfrac{1}{2L_{\\max}} & \\mbox{for}\\quad t \\leq 4\\lceil\\mathcal{K} \\rceil \\\\[0.3cm]\n",
    "\\displaystyle \\tfrac{2t+1}{(t+1)^2 \\mu} &  \\mbox{for}\\quad t > 4\\lceil\\mathcal{K} \\rceil.\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with a switching stepsizes. Please name the output as\n",
    "# w_sgdsr, obj_sgdss, err_sgdss = sgd(.....?.....)\n",
    "##############END TODO######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdss - obj_min, label=\"SGD switch\", lw=2)\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Error of objective\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdss , label=\"SGD switch\", lw=2)\n",
    "plt.semilogy(err_sgdsr , label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with averaging step\n",
    "\n",
    "- Implement the average iterate output  \n",
    "- Compare the solution you obtain for SGD with shrinking stepsizes and SGD with averaging. \n",
    "- What happens if you start averaging only the last n iterates? When is averaging useful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with averaging on and shrinking stepsize. Please name the output as\n",
    "# w_sgdar, obj_sgdar, err_sgdar = sgd( .... )\n",
    "# HINT: you can use the same stepsizes as decreasing and average just the last n_iter/4 steps\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdss - obj_min, label=\"SGD switch\", lw=2)\n",
    "plt.semilogy(obj_sgdar - obj_min, label=\"SGD average end\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Loss function\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(err_sgdss , label=\"SGD switch\", lw=2)\n",
    "plt.semilogy(err_sgdar , label=\"SGD average end\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with gradient descent\n",
    "\n",
    "- Complete the code of gradient descent (GD) below\n",
    "- How much more is the computational cost of a step of gradient descent with respect to the computational cost of a SGD step?  How many steps of gradient descent should you take so that the total computational complexity is equivalent to datapasses * n steps of SGD ? \n",
    "- Compare GD with SGD, where on the $x$-axis of the plot you the total computational effort spent\n",
    "- What happens if you increase the number of datapasses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(w0, model, step, w_min =[], n_iter=100, verbose=True):\n",
    "    \"\"\"Gradient descent algorithm\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    n_samples, n_features = X.shape\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1.\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    if np.any(w_min):\n",
    "        err = norm(w - w_min) / norm(w_min)\n",
    "        errors.append(err)\n",
    "    # Current objective\n",
    "    obj = model.f(w) \n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter ):\n",
    "        ##### TODO ######################\n",
    "        ##### Compute gradient step update\n",
    "        #####   w[:] = ...\n",
    "        ##### END TODO ##################\n",
    "        obj = model.f(w) \n",
    "        if (sum(w_min)):\n",
    "            err = norm(w - w_min) / norm(w_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return w, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1. / model.lipschitz_constant()\n",
    "w_gd, obj_gd, err_gd = gd(w0, model, step, w_min, datapasses)\n",
    "print(obj_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexityofGD = n * np.arange(0, datapasses + 1)\n",
    "print(complexityofGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(complexityofGD, obj_gd - obj_min, label=\"gd\", lw=2)\n",
    "plt.semilogy(obj_sgdss - obj_min, label=\"sgd switch\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"# SGD iterations\", fontsize=14)\n",
    "plt.ylabel(\"Loss function\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimum on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(complexityofGD, err_gd, label=\"gd\", lw=2)\n",
    "plt.semilogy(err_sgdss , label=\"sgd switch\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"# SGD iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: SGD without replacement\n",
    "\n",
    "-Execute SGD where the indices of the data points are sampled *without* replacement over each datapass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############TODO#######################\n",
    "# Execute SGD with averaging on and shrinking stepsize. Please name the output as\n",
    "# w_sgdsw, obj_sgdsw, err_sgdsw = sgd(....)\n",
    "# HINT: You should use numpy.matlib's repmat function to  \n",
    "#import numpy.matlib\n",
    "# With replacement\n",
    "#indices = np.matlib.repmat(... )\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(obj_sgdss - obj_min, label=\"SGD switch replace\", lw=2)\n",
    "plt.plot(obj_sgdsw - obj_min, label=\"SGD withoutreplace\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(err_sgdss , label=\"SGD replace\", lw=2)\n",
    "plt.plot(err_sgdsw , label=\"SGD withoutreplace\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdsw , label=\"SGD withoutreplace\", lw=2)\n",
    "plt.semilogy(err_sgdsaw , label=\"SGD averaging end\", lw=2)\n",
    "plt.semilogy(complexityofGD, err_gd , label=\"GD\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to true model\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare SGD and GD in terms of test error. That is, use w_model_truth to compareÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapasses = 30;\n",
    "n_iters = int(datapasses * n)\n",
    "\n",
    "\n",
    "## SGD without replacement, decreasing stepsize, averaging at end\n",
    "#############TODO#######################\n",
    "# w_sgdar, obj_sgdar, err_sgdar    = sgd(... w_model_truth ...) \n",
    "###############END TODO#####################\n",
    "\n",
    "\n",
    "\n",
    "## SGD without replacement, decreasing stepsize, no averaging\n",
    "#############TODO#######################\n",
    "#  w_sgdsw, obj_sgdsw, err_sgdsw = sgd( ...  w_model_truth ...);\n",
    "###############END TODO#####################\n",
    "\n",
    "## GD\n",
    "#############TODO#######################\n",
    "# w_gd, obj_gd, err_gd = gd( ...  w_model_truth ... )\n",
    "###############END TODO#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexityofGD = n * np.arange(0, datapasses + 1)\n",
    "\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.semilogy(err_sgdsw , label=\"SGD withoutreplace\", lw=2)\n",
    "plt.semilogy(err_sgdar , label=\"SGD averaging end\", lw=2)\n",
    "plt.semilogy(complexityofGD, err_gd , label=\"GD\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to true model\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
