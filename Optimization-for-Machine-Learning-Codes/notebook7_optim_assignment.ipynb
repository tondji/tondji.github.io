{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First order methods for regression models\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- gradient descent (GD)\n",
    "- accelerated gradient descent (AGD)\n",
    "- coordinate gradient descent (CD)\n",
    "- stochastic gradient descent (SGD)\n",
    "- stochastic variance reduced gradient descent (SVRG)\n",
    "\n",
    "\n",
    "for the linear regression and logistic regression models, with the \n",
    "ridge penalization.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "## To generate the name of your file, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using your first and last names\n",
    "fn1 = \"bonnie\"\n",
    "ln1 = \"parker\"\n",
    "fn2 = \"clyde\"\n",
    "ln2 = \"barrow\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "[1. Introduction](#intro)<br>\n",
    "[2. Models gradients and losses](#models)<br>\n",
    "[3. Solvers](#solvers)<br>\n",
    "[4. Comparison of all algorithms](#comparison)<br>\n",
    "\n",
    "<a id='intro'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Getting model weights\n",
    "\n",
    "We'll start by generating sparse vectors and simulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style='white')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Simulation of a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(w0, n_samples=1000, corr=0.5, std=0.5):\n",
    "    \"\"\"Simulation of a linear regression model with Gaussian features\n",
    "    and a Toeplitz covariance, with Gaussian noise.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "    \n",
    "    std : `float`, default=0.5\n",
    "        Standard deviation of the noise\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian  vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    y = X.dot(w0) + std * randn(n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "w0 = np.array([0.5])\n",
    "\n",
    "X, y = simu_linreg(w0, n_samples=n_samples, corr=0.3, std=0.5)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(r\"$x_i$\", fontsize=16)\n",
    "plt.ylabel(r\"$y_i$\", fontsize=16)\n",
    "plt.title(\"Linear regression simulation\", fontsize=18)\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Simulation of a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function (overflow-proof)\"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size)    \n",
    "    out[idx] = 1 / (1. + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def simu_logreg(w0, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a logistic regression model with Gaussian features\n",
    "    and a Toeplitz covariance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w0 : `numpy.array`, shape=(n_features,)\n",
    "        Model weights\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It contains samples of a centered \n",
    "        Gaussian vector with Toeplitz covariance.\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    n_features = w0.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(X.dot(w0))\n",
    "    y = np.random.binomial(1, p, size=n_samples)\n",
    "    # Put the label in {-1, 1}\n",
    "    y[:] = 2 * y - 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "w0 = np.array([-3, 3.])\n",
    "\n",
    "X, y = simu_logreg(w0, n_samples=n_samples, corr=0.4)\n",
    "\n",
    "plt.scatter(*X[y == 1].T, label=r'$y_i=1$')\n",
    "plt.scatter(*X[y == -1].T, label=r'$y_i=-1$')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(r\"$x_i^1$\", fontsize=16)\n",
    "plt.ylabel(r\"$x_i^2$\", fontsize=16)\n",
    "plt.title(\"Logistic regression simulation\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "# 2. Models gradients and losses\n",
    "\n",
    "We want to minimize a goodness-of-fit function $f$ with ridge regularization, namely\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\n",
    "$$\n",
    "where $d$ is the number of features and where we will assume that $f$ is $L$-smooth.\n",
    "We will consider below the following cases.\n",
    "\n",
    "**Linear regression**, where \n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - x_i^\\top w)^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 = \\frac{1}{2 n} \\| y - X w \\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, $y = [y_1 \\cdots y_n]$ is the vector of labels and $X$ is the matrix of features with lines containing the features vectors $x_i \\in \\mathbb R^d$.\n",
    "\n",
    "**Logistic regression**, where\n",
    "$$\n",
    "f(w) = \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + \\exp(-y_i x_i^\\top w)) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, and where labels $y_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We need to be able to compute $f(w)$ and its gradient $\\nabla f(w)$, in order to solve this problem, as well as $\\nabla f_i(w)$ for stochastic gradient descent methods and $\\frac{\\partial f(w)}{\\partial w_j}$ for coordinate descent.\n",
    "\n",
    "Below is the full implementation for linear regression.\n",
    "\n",
    "## 2.1 Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class ModelLinReg:\n",
    "    \"\"\"A class giving first order information for linear regression\n",
    "    with least-squares loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return 0.5 * norm(y - X.dot(w)) ** 2 / n_samples + strength * norm(w) ** 2 / 2\n",
    "    \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X.T.dot(X.dot(w) - y) / n_samples + strength * w\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i]\n",
    "        return (x_i.dot(w) - y[i]) * x_i + self.strength * w\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        return X[:, j].T.dot(X.dot(w) - y) / n_samples + strength * w[j]\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return norm(X.T.dot(X), 2) / n_samples + self.strength\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return (X ** 2).sum(axis=0) / n_samples + self.strength\n",
    "        \n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        return ((X ** 2).sum(axis=1) + self.strength).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checks for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation setting\n",
    "n_features = 50\n",
    "nnz = 20\n",
    "idx = np.arange(n_features, dtype=np.double)\n",
    "w0 = (-1) ** (idx - 1) * np.exp(-idx / 10.)\n",
    "w0[nnz:] = 0.\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.stem(w0)\n",
    "plt.title(\"Model weights\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "X, y = simu_linreg(w0, corr=0.6)\n",
    "model = ModelLinReg(X, y, strength=1e-3)\n",
    "w = np.random.randn(n_features)\n",
    "\n",
    "print(check_grad(model.loss, model.grad, w)) # This must be a number (of order 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"lip=\", model.lip())\n",
    "print(\"lip_max=\", model.lip_max())\n",
    "print(\"lip_coordinates=\", model.lip_coordinates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression\n",
    "\n",
    "**NB**: you can skip these questions and go to the solvers implementation, and come back here later.\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Compute (on paper) the gradient $\\nabla f$, the gradient of $\\nabla f_i$ and the gradient of the coordinate function $\\frac{\\partial f(w)}{\\partial w_j}$ of $f$ for logistic regression (fill the class given below).\n",
    "\n",
    "2. Fill in the functions below for the computation of $f$, $\\nabla f$, $\\nabla f_i$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for logistic regression in the ModelLogReg class below (fill between the TODO and END TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLogReg:\n",
    "    \"\"\"A class giving first order information for logistic regression\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : `numpy.array`, shape=(n_samples, n_features)\n",
    "        The features matrix\n",
    "    \n",
    "    y : `numpy.array`, shape=(n_samples,)\n",
    "        The vector of labels\n",
    "    \n",
    "    strength : `float`\n",
    "        The strength of ridge penalization\n",
    "    \"\"\"    \n",
    "    def __init__(self, X, y, strength):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.strength = strength\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        \"\"\"Computes f(w)\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "       \n",
    "    def grad(self, w):\n",
    "        \"\"\"Computes the gradient of f at w\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_i(self, i, w):\n",
    "        \"\"\"Computes the gradient of f_i at w\"\"\"\n",
    "        x_i = self.X[i], strength = self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        \"\"\"Computes the partial derivative of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        y, X, n_samples, strength = self.y, self.X, self.n_samples, self.strength\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_coordinates(self):\n",
    "        \"\"\"Computes the Lipschitz constant of f with respect to \n",
    "        the j-th coordinate\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def lip_max(self):\n",
    "        \"\"\"Computes the maximum of the lipschitz constants of f_i\"\"\"\n",
    "        X, n_samples = self.X, self.n_samples\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Checks for the logistic regression model\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Check numerically the gradient using the function ``checkgrad`` from ``scipy.optimize`` (see below), as we did for linear regression above\n",
    "\n",
    "**Remark**: use the function `simu_logreg` to simulate data according to the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='solvers'></a>\n",
    "## 3. Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have classes `ModelLinReg` and `ModelLogReg` that allow to compute $f(w)$, $\\nabla f(w)$, \n",
    "$\\nabla f_i(w)$ and $\\frac{\\partial f(w)}{\\partial w_j}$ for the objective $f$\n",
    "given by linear and logistic regression.\n",
    "\n",
    "We want now to code and compare several solvers to minimize $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 3.1. Tools for the solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point of all solvers\n",
    "w0 = np.zeros(model.n_features)\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 50\n",
    "\n",
    "# Random samples indices for the stochastic solvers (sgd, sag, svrg)\n",
    "idx_samples = np.random.randint(0, model.n_samples, model.n_samples * n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspector(model, n_iter, verbose=True):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\n",
    "    Don't even look at it, we'll just use it in the solvers.\"\"\"\n",
    "    objectives = []\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    def inspector_cl(w):\n",
    "        obj = model.loss(w)\n",
    "        objectives.append(obj)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.objectives = objectives\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gd'></a>\n",
    "## 3.2 Gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `gd` below that implements the gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_gd = inspector(model, n_iter=n_iter)\n",
    "w_gd = gd(model, w0, n_iter=n_iter, callback=callback_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='agd'></a>\n",
    "## 3.3 Accelerated gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `agd` below that implements the accelerated gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Accelerated gradient descent\n",
    "    \"\"\"\n",
    "    step = 1 / model.lip()\n",
    "    w = w0.copy()\n",
    "    w_new = w0.copy()\n",
    "    # An extra variable is required for acceleration\n",
    "    z = w0.copy()\n",
    "    t = 1.\n",
    "    t_new = 1.    \n",
    "    if verbose:\n",
    "        print(\"Lauching AGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_agd = inspector(model, n_iter=n_iter)\n",
    "w_agd = agd(model, w0, n_iter=n_iter, callback=callback_agd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cgd'></a>\n",
    "\n",
    "## 3.4 Coordinate gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `cgd` below that implements the coordinate gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cgd(model, w0, n_iter, callback, verbose=True):\n",
    "    \"\"\"Coordinate gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_features = model.n_features\n",
    "    steps = 1 / model.lip_coordinates()\n",
    "    if verbose:\n",
    "        print(\"Lauching CGD solver...\")\n",
    "    callback(w)\n",
    "    for k in range(n_iter + 1):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_cgd = inspector(model, n_iter=n_iter)\n",
    "w_cgd = cgd(model, w0, n_iter=n_iter, callback=callback_cgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sgd'></a>\n",
    "## 3.5. Stochastic gradient descent\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `sgd` below that implements the st stochastic gradient descent algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    callback(w)\n",
    "    n_samples = model.n_samples\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1e-1\n",
    "callback_sgd = inspector(model, n_iter=n_iter)\n",
    "w_sgd = sgd(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sag'></a>\n",
    "## 3.6. Stochastic average gradient descent\n",
    "\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "1. Finish the function `sag` below that implements the stochastic averaged gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sag(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic average gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    n_samples, n_features = model.n_samples, model.n_features\n",
    "    gradient_memory = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_features)\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):\n",
    "        i = idx_samples[idx]        \n",
    "        ### TODO\n",
    "\n",
    "        ### END OF TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_sag = inspector(model, n_iter=n_iter)\n",
    "w_sag = sag(model, w0, idx_samples, n_iter=model.n_samples * n_iter, \n",
    "            step=step, callback=callback_sag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svrg'></a>\n",
    "## 3.7. Stochastic variance reduced gradient\n",
    "\n",
    "### QUESTIONS\n",
    "\n",
    "- Finish the function `svrg` below that implements the stochastic variance reduced gradient algorithm\n",
    "- Test it using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n",
    "    \"\"\"Stochastic variance reduced gradient descent\n",
    "    \"\"\"\n",
    "    w = w0.copy()\n",
    "    w_old = w.copy()\n",
    "    n_samples = model.n_samples\n",
    "    callback(w)\n",
    "    for idx in range(n_iter):        \n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO        \n",
    "        if idx % n_samples == 0:\n",
    "            callback(w)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 1 / model.lip_max()\n",
    "callback_svrg = inspector(model, n_iter=n_iter)\n",
    "w_svrg = svrg(model, w0, idx_samples, n_iter=model.n_samples * n_iter,\n",
    "              step=step, callback=callback_svrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "# 4. Comparison of all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_gd, callback_agd, callback_cgd, callback_sgd, \n",
    "             callback_sag, callback_svrg]\n",
    "names = [\"GD\", \"AGD\", \"CGD\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "callback_long = inspector(model, n_iter=1000, verbose=False)\n",
    "w_cgd = cgd(model, w0, n_iter=1000, callback=callback_long, verbose=False)\n",
    "obj_min = callback_long.objectives[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "for callback, name in zip(callbacks, names):\n",
    "    objectives = np.array(callback.objectives)\n",
    "    objectives_dist = objectives - obj_min    \n",
    "    plt.plot(objectives_dist, label=name, lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim((0, n_iter))\n",
    "plt.xlabel(\"Number of passes on the data\", fontsize=16)\n",
    "plt.ylabel(r\"$F(w^k) - F(w^*)$\", fontsize=16)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "\n",
    "1. Compare the minimizers you obtain using the different algorithms, with a large and a small number of iterations. This can be done with `plt.stem` plots.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the correlation \n",
    "  of the features on the performance of the optimization algorithms. Explain.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.\n",
    "- (OPTIONAL) All algorithms can be modified to handle an objective of the form $f + g$ with $g$ separable and prox-capable. Modify all the algorithms and try them out for L1 penalization $f(w) = \\lambda \\sum_{j=1}^d |w_j|$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
