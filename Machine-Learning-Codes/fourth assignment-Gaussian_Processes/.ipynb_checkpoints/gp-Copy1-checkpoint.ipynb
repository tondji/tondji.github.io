{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# LoadData takes the file location for the yacht_hydrodynamics.data and returns\n",
    "# the data set partitioned into a training set and a test set.\n",
    "# the X matrix, deal with the month and day strings.\n",
    "# Do not change this function!\n",
    "# ##############################################################################\n",
    "def loadData(df):\n",
    "    data = np.loadtxt(df)\n",
    "    Xraw = data[:,:-1]\n",
    "    # The regression task is to predict the residuary resistance per unit weight of displacement\n",
    "    yraw = (data[:,-1])[:, None]\n",
    "    X = (Xraw-Xraw.mean(axis=0))/np.std(Xraw, axis=0)\n",
    "    y = (yraw-yraw.mean(axis=0))/np.std(yraw, axis=0)\n",
    "\n",
    "    ind = range(X.shape[0])\n",
    "    test_ind = ind[0::4] # take every fourth observation for the test set\n",
    "    train_ind = list(set(ind)-set(test_ind))\n",
    "    X_test = X[test_ind]\n",
    "    X_train = X[train_ind]\n",
    "    y_test = y[test_ind]\n",
    "    y_train = y[train_ind]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# Returns a single sample from a multivariate Gaussian with mean and cov.\n",
    "# ##############################################################################\n",
    "def multivariateGaussianDraw(mean, cov):\n",
    "    \n",
    "    #n = mean.shape[0]\n",
    "    #sample = np.zeros((mean.shape[0], 1)) # This is only a placeholder\n",
    "    # Task 2:\n",
    "    # TODO: Implement a draw from a multivariate Gaussian here\n",
    "    \n",
    "#     K = np.linalg.cholesky(cov + 1e-15*np.eye(n))\n",
    "#     sample = np.dot(L, np.random.normal(size=(n,1)))\n",
    "    sample = np.random.multivariate_normal(mean, cov, 5)\n",
    "\n",
    "    # Return drawn sample\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# RadialBasisFunction for the kernel function\n",
    "# k(x,x') = s2_f*exp(-norm(x,x')^2/(2l^2)). If s2_n is provided, then s2_n is\n",
    "# added to the elements along the main diagonal, and the kernel function is for\n",
    "# the distribution of y,y* not f, f*.\n",
    "# ##############################################################################\n",
    "class RadialBasisFunction():\n",
    "    def __init__(self, params):\n",
    "        self.ln_sigma_f = params[0]\n",
    "        self.ln_length_scale = params[1]\n",
    "        self.ln_sigma_n = params[2]\n",
    "\n",
    "        self.sigma2_f = np.exp(2*self.ln_sigma_f)\n",
    "        self.sigma2_n = np.exp(2*self.ln_sigma_n)\n",
    "        self.length_scale = np.exp(self.ln_length_scale)\n",
    "\n",
    "    def setParams(self, params):\n",
    "        self.ln_sigma_f = params[0]\n",
    "        self.ln_length_scale = params[1]\n",
    "        self.ln_sigma_n = params[2]\n",
    "\n",
    "        self.sigma2_f = np.exp(2*self.ln_sigma_f)\n",
    "        self.sigma2_n = np.exp(2*self.ln_sigma_n)\n",
    "        self.length_scale = np.exp(self.ln_length_scale)\n",
    "\n",
    "    def getParams(self):\n",
    "        return np.array([self.ln_sigma_f, self.ln_length_scale, self.ln_sigma_n])\n",
    "\n",
    "    def getParamsExp(self):\n",
    "        return np.array([self.sigma2_f, self.length_scale, self.sigma2_n])\n",
    "\n",
    "    # ##########################################################################\n",
    "    # covMatrix computes the covariance matrix for the provided matrix X using\n",
    "    # the RBF. If two matrices are provided, for a training set and a test set,\n",
    "    # then covMatrix computes the covariance matrix between all inputs in the\n",
    "    # training and test set.\n",
    "    # ##########################################################################\n",
    "    def covMatrix(self, X, Xa=None):\n",
    "        if Xa is not None:\n",
    "            X_aug = np.zeros((X.shape[0]+Xa.shape[0], X.shape[1]))\n",
    "            X_aug[:X.shape[0], :X.shape[1]] = X\n",
    "            X_aug[X.shape[0]:, :X.shape[1]] = Xa\n",
    "            X=X_aug\n",
    "\n",
    "        n = X.shape[0]\n",
    "        covMat = np.zeros((n,n))\n",
    "\n",
    "        # Task 1:\n",
    "        # TODO: Implement the covariance matrix here\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                sqdist = np.exp(-0.5 * (1/(self.length_scale)**2)* (np.linalg.norm(X[i,:]-X[j,:]))**2)\n",
    "                covMat[i,j] = self.sigma2_f *  sqdist\n",
    "\n",
    "\n",
    "        # If additive Gaussian noise is provided, this adds the sigma2_n along\n",
    "        # the main diagonal. So the covariance matrix will be for [y y*]. If\n",
    "        # you want [y f*], simply subtract the noise from the lower right\n",
    "        # quadrant.\n",
    "        if self.sigma2_n is not None:\n",
    "            covMat += self.sigma2_n*np.identity(n)\n",
    "\n",
    "        # Return computed covariance matrix\n",
    "        return covMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ln_sigma_f = np.log(1.27)\n",
    "ln_length_scale = np.log(1)\n",
    "ln_sigma_n = np.log(0.3)\n",
    "basis = RadialBasisFunction([ln_sigma_f,ln_length_scale,ln_sigma_n])\n",
    "X = np.array([-1.50,-1.00,-0.7,-0.40, -0.25, 0.00]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027966044546840325"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(basis.covMatrix(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = loadData('yacht_hydrodynamics.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basis = RadialBasisFunction([ 1.10442998 ,17.38230593 ,-10.10133779])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(basis.covMatrix(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianProcessRegression():\n",
    "    def __init__(self, X, y, k):\n",
    "        self.X = X\n",
    "        self.n = X.shape[0]\n",
    "        self.y = y\n",
    "        self.k = k\n",
    "        self.K = self.KMat(self.X)\n",
    "        self.L = np.linalg.cholesky(self.K)\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Recomputes the covariance matrix and the inverse covariance\n",
    "    # matrix when new hyperparameters are provided.\n",
    "    # ##########################################################################\n",
    "    def KMat(self, X, params=None):\n",
    "        if params is not None:\n",
    "            self.k.setParams(params)\n",
    "        K = self.k.covMatrix(X)\n",
    "        self.K = K\n",
    "        self.L = np.linalg.cholesky(self.K)\n",
    "        return K\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Computes the posterior mean of the Gaussian process regression and the\n",
    "    # covariance for a set of test points.\n",
    "    # NOTE: This should return predictions using the 'clean' (not noisy) covariance\n",
    "    # ##########################################################################\n",
    "    def predict(self, Xa):\n",
    "        mean_fa = np.zeros((Xa.shape[0], 1))\n",
    "        cov_fa = np.zeros((Xa.shape[0], Xa.shape[0]))\n",
    "        # Task 3:\n",
    "        # TODO: compute the mean and covariance of the prediction\n",
    "        \n",
    "        \n",
    "        #beginning of my solution: ------->\n",
    "        \n",
    "        # Covariance between training sample points (without Gaussian noise)\n",
    "        Kxx = self.KMat(self.X) # + 1 * np.eye(10) if there is  Gaussian noise\n",
    "        \n",
    "        # Covariance between training and test points\n",
    "        Kxs = self.KMat(self.Xa, self.X)\n",
    "        \n",
    "        # Covariance between test points\n",
    "        Kss = self.KMat(self.Xa, self.Xa)\n",
    "        \n",
    "        # The mean of the GP fit (note that @ is matrix multiplcation: A @ B is equivalent to np.matmul(A,B))\n",
    "        mean_fa = Kxs @ np.linalg.inv(Kxx) @ self.y\n",
    "        # The covariance matrix of the GP fit\n",
    "        cov_fa = Kss - Kxs @ np.linalg.inv(Kxx) @ Kxs.T\n",
    "        \n",
    "\n",
    "        # Return the mean and covariance\n",
    "        return mean_fa, cov_fa\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Return negative log marginal likelihood of training set. Needs to be\n",
    "    # negative since the optimiser only minimises.\n",
    "    # ##########################################################################\n",
    "    def logMarginalLikelihood(self, params=None):\n",
    "        if params is not None:\n",
    "            self.KMat(self.X, params)\n",
    "\n",
    "        mll = 0\n",
    "        # Task 4:\n",
    "        # TODO: Calculate the log marginal likelihood ( mll ) of self.y\n",
    "        \n",
    "        mll = 0.5* (self.y).T @ np.linalg.solve((self.L).T, np.linalg.solve(self.L, self.y)) + 0.5 * (np.product((self.L).diagonal()))**2 + 0.5 * self.n * np.log(2 * np.pi)\n",
    "\n",
    "        # Return mll\n",
    "        return mll\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Computes the gradients of the negative log marginal likelihood wrt each\n",
    "    # hyperparameter.\n",
    "    # ##########################################################################\n",
    "    def gradLogMarginalLikelihood(self, params=None):\n",
    "        if params is not None:\n",
    "            K = self.KMat(self.X, params)\n",
    "\n",
    "        grad_ln_sigma_f = grad_ln_length_scale = grad_ln_sigma_n = 0\n",
    "        # Task 5:\n",
    "        # TODO: calculate the gradients of the negative log marginal likelihood\n",
    "        # wrt. the hyperparameters\n",
    "        \n",
    "        beta_ln_length_scale = np.zeros((self.n, self.n))\n",
    "        beta_ln_sigma_f = np.zeros((self.n, self.n))\n",
    "        beta_ln_sigma_n = np.zeros((self.n, self.n))\n",
    "        \n",
    "        param = self.k.getParamsExp()\n",
    "        \n",
    "        \n",
    "        alpha = np.linalg.solve(self.K, self.y)\n",
    "        inv = np.linalg.solve((self.L).T, np.linalg.solve(self.L, np.eye(self.n,self.n)))\n",
    "        L = self.L\n",
    "        for i in range(self.n):\n",
    "            for j in range(n):\n",
    "                beta_ln_length_scale[i,j] = (1/param[0]) * ((X[i]-X[j])**2) * param[2] * np.exp(-0.5*((X[i]-X[j])**2)/param[0]) \n",
    "                beta_ln_sigma_n[i,j] =  2 * param[1]\n",
    "                beta_ln_sigma_f[i,j] = 2 * param[2] * (np.exp(-0.5*((X[i]-X[j])**2)/param[0])) * \n",
    "        \n",
    "        grad_ln_sigma_f = -0.5 * np.trace((alpha @ alpha.T - inv) @ beta_ln_sigma_f)\n",
    "        grad_ln_length_scale = -0.5 * np.trace((alpha @ alpha.T - inv) @ beta_ln_length_scale)\n",
    "        grad_ln_sigma_n = -0.5 * np.trace((alpha @ alpha.T - inv) @ beta_ln_sigma_n)\n",
    "        \n",
    "\n",
    "        # Combine gradients\n",
    "        gradients = np.array([grad_ln_sigma_f, grad_ln_length_scale, grad_ln_sigma_n])\n",
    "\n",
    "        # Return the gradients\n",
    "        return gradients\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Computes the mean squared error between two input vectors.\n",
    "    # ##########################################################################\n",
    "    def mse(self, ya, fbar):\n",
    "        mse = 0\n",
    "        # Task 7:\n",
    "        # TODO: Implement the MSE between ya and fbar\n",
    "        \n",
    "        mse = np.sum((ya[i,:] - fbar[i,:])**2)/(ya.shape[0])\n",
    "\n",
    "        # Return mse\n",
    "        return mse\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Computes the mean standardised log loss.\n",
    "    # ##########################################################################\n",
    "    def msll(self, ya, fbar, cov):\n",
    "        msll = 0\n",
    "        # Task 7:\n",
    "        # TODO: Implement MSLL of the prediction fbar, cov given the target ya\n",
    "\n",
    "        return msll\n",
    "\n",
    "    # ##########################################################################\n",
    "    # Minimises the negative log marginal likelihood on the training set to find\n",
    "    # the optimal hyperparameters using BFGS.\n",
    "    # ##########################################################################\n",
    "    def optimize(self, params, disp=True):\n",
    "        \n",
    "        #params = np.array([0.5*np.log(1.0), np.log(1.0), 0.5*np.log(0.5)])\n",
    "        res = minimize(self.logMarginalLikelihood, params, method ='BFGS', jac = self.gradLogMarginalLikelihood, options = {'disp':disp})\n",
    "        return res.x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ##########################\n",
    "    # You can put your tests here - marking\n",
    "    # will be based on importing this code and calling\n",
    "    # specific functions with custom input.\n",
    "    ##########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
