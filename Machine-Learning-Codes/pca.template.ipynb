{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pca.template.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"Q490KkHtE9pS","colab_type":"text"},"cell_type":"markdown","source":["# Principal Component Analysis (PCA)\n","We will implement the PCA algorithm. We will first implement PCA, then apply it (once again) to the MNIST digit dataset."]},{"metadata":{"id":"9oyeoF7ZE9pT","colab_type":"text"},"cell_type":"markdown","source":["## Learning objective\n","1. Write code that implements PCA.\n","2. Write code that implements PCA for high-dimensional datasets"]},{"metadata":{"id":"XAt6ZjLGE9pY","colab_type":"code","colab":{}},"cell_type":"code","source":["# PACKAGE: DO NOT EDIT\n","import numpy as np\n","import timeit"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8eiUAR9JE9pc","colab_type":"code","colab":{}},"cell_type":"code","source":["# PACKAGE: DO NOT EDIT\n","import matplotlib as mpl\n","mpl.use('Agg')\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","from ipywidgets import interact\n","\n","from sklearn.datasets import fetch_mldata\n","MNIST = fetch_mldata('MNIST original', data_home='./MNIST')\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jkuIIyahE9pf","colab_type":"text"},"cell_type":"markdown","source":["Now, let's plot a digit from the dataset:"]},{"metadata":{"id":"jW8kHiGtE9ph","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(4,4))\n","plt.imshow(MNIST.data[0].reshape(28,28), cmap='gray');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1iYOfY4kE9pk","colab_type":"text"},"cell_type":"markdown","source":["Before we implement PCA, we will need to do some data preprocessing. In this assessment, some of them \n","will be implemented by you, others we will take care of. However, when you are working on real world problems, you will need to do all these steps by yourself!\n","\n","The preprocessing steps we will do are\n","1. Convert unsigned interger 8 (uint8) encoding of pixels to a floating point number between 0-1.\n","2. Subtract from each image the mean $\\boldsymbol \\mu$.\n","3. Scale each dimension of each image by $\\frac{1}{\\sigma}$ where $\\sigma$ is the stardard deviation.\n","\n","The steps above ensure that our images will have zero mean and one variance. These preprocessing\n","steps are also known as [Data Normalization or Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)."]},{"metadata":{"id":"vOWqt6amE9pn","colab_type":"text"},"cell_type":"markdown","source":["## 1. PCA\n","\n","Now we will implement PCA. Before we do that, let's pause for a moment and\n","think about the steps for performing PCA. Assume that we are performing PCA on\n","some dataset $\\boldsymbol X$ for $M$ principal components. \n","We then need to perform the following steps, which we break into parts:\n","\n","1. Data normalization (`normalize`).\n","2. Find eigenvalues and corresponding eigenvectors for the covariance matrix $S$.\n","   Sort by the largest eigenvalues and the corresponding eigenvectors (`eig`).\n","\n","After these steps, we can then compute the projection and reconstruction of the data onto the spaced spanned by the top $n$ eigenvectors."]},{"metadata":{"id":"kO5IiW8WE9po","colab_type":"code","colab":{}},"cell_type":"code","source":["# GRADED FUNCTION: DO NOT EDIT\n","\n","def normalize(X):\n","    \"\"\"Normalize the given dataset X\n","    Args:\n","        X: ndarray, dataset\n","    \n","    Returns:\n","        (Xbar, mean, std): ndarray, Xbar is the normalized dataset\n","        with mean 0 and standard deviation 1; mean and std are the \n","        mean and standard deviation respectively.\n","    \n","    Note:\n","        You will encounter dimensions where the standard deviation is\n","        zero, for those when you do normalization the normalized data\n","        will be NaN. Handle this by setting using `std = 1` for those \n","        dimensions when doing normalization.\n","    \"\"\"\n","    mu = np.zeros(X.shape[1]) # <-- EDIT THIS\n","    std = np.std(X, axis=0)\n","    std_filled = std.copy()\n","    std_filled[std==0] = 1.\n","    Xbar = X                  # <-- EDIT THIS\n","\n","def eig(S):\n","    \"\"\"Compute the eigenvalues and corresponding eigenvectors \n","        for the covariance matrix S.\n","    Args:\n","        S: ndarray, covariance matrix\n","    \n","    Returns:\n","        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors\n","\n","    Note:\n","        the eigenvals and eigenvecs should be sorted in descending\n","        order of the eigen values\n","    \"\"\"\n","    return (None, None) # <-- EDIT THIS\n","\n","def projection_matrix(B):\n","    \"\"\"Compute the projection matrix onto the space spanned by `B`\n","    Args:\n","        B: ndarray of dimension (D, M), the basis for the subspace\n","    \n","    Returns:\n","        P: the projection matrix\n","    \"\"\"\n","    return np.eye(B.shape[0]) # <-- EDIT THIS\n","\n","def PCA(X, num_components):\n","    \"\"\"\n","    Args:\n","        X: ndarray of size (N, D), where D is the dimension of the data,\n","           and N is the number of datapoints\n","        num_components: the number of principal components to use.\n","    Returns:\n","        X_reconstruct: a tuple where the first item is the reconstruction\n","        of X from the first `num_components` principal components.\n","    \"\"\"\n","    return X # <-- EDIT THIS\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jp9vzSwRE9pr","colab_type":"code","colab":{}},"cell_type":"code","source":["## Some preprocessing of the data\n","NUM_DATAPOINTS = 1000\n","X = (MNIST.data.reshape(-1, 28 * 28)[:NUM_DATAPOINTS]) / 255.\n","Xbar, mu, std = normalize(X)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GoXyqDZUE9pv","colab_type":"code","colab":{}},"cell_type":"code","source":["for num_component in range(1, 20):\n","    from sklearn.decomposition import PCA as SKPCA\n","    # We can compute a standard solution given by scikit-learn's implementation of PCA\n","    pca = SKPCA(n_components=num_component, svd_solver='full')\n","    sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))\n","    reconst = PCA(Xbar, num_component)\n","    np.testing.assert_almost_equal(reconst, sklearn_reconst)\n","    print(np.square(reconst - sklearn_reconst).sum())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JaoCHnzrE9py","colab_type":"text"},"cell_type":"markdown","source":["The greater number of of principal components we use, the smaller will our reconstruction\n","error be. Now, let's answer the following question: \n","\n","\n","> How many principal components do we need\n","> in order to reach a Mean Squared Error (MSE) of less than $100$ for our dataset?\n","\n"]},{"metadata":{"id":"Rg2rn-sME9p0","colab_type":"code","colab":{}},"cell_type":"code","source":["def mse(predict, actual):\n","    return np.square(predict - actual).sum(axis=1).mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A7AAFKMbE9p6","colab_type":"code","colab":{}},"cell_type":"code","source":["loss = []\n","reconstructions = []\n","for num_component in range(1, 100):\n","    reconst = PCA(Xbar, num_component)\n","    error = mse(reconst, Xbar)\n","    reconstructions.append(reconst)\n","    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n","    loss.append((num_component, error))\n","\n","reconstructions = np.asarray(reconstructions)\n","reconstructions = reconstructions * std + mu # \"unnormalize\" the reconstructed image\n","loss = np.asarray(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xp83JCQ_E9qC","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","pd.DataFrame(loss).head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SmTGLJf7E9qG","colab_type":"text"},"cell_type":"markdown","source":["We can also put these numbers into perspective by plotting them."]},{"metadata":{"id":"_RLMMmz_E9qI","colab_type":"code","colab":{}},"cell_type":"code","source":["fig, ax = plt.subplots()\n","ax.plot(loss[:,0], loss[:,1]);\n","ax.axhline(100, linestyle='--', color='r', linewidth=2)\n","ax.xaxis.set_ticks(np.arange(1, 100, 5));\n","ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5J5CprdaE9qN","colab_type":"text"},"cell_type":"markdown","source":["But _numbers dont't tell us everything_! Just what does it mean _qualitatively_ for the loss to decrease from around\n","$450.0$ to less than $100.0$?\n","\n","Let's find out! In the next cell, we draw the the leftmost image is the original dight. Then we show the reconstruction of the image on the right, in descending number of principal components used."]},{"metadata":{"id":"kZLqShKDE9qR","colab_type":"code","colab":{}},"cell_type":"code","source":["@interact(image_idx=(0, 1000))\n","def show_num_components_reconst(image_idx):\n","    fig, ax = plt.subplots(figsize=(20., 20.))\n","    actual = X[image_idx]\n","    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n","    ax.imshow(np.hstack(x.reshape(-1, 28, 28)[np.arange(10)]),\n","              cmap='gray');\n","    ax.axvline(28, color='orange', linewidth=2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qnfS8DuPE9qZ","colab_type":"text"},"cell_type":"markdown","source":["We can also browse throught the reconstructions for other digits. Once again, `interact` becomes handy."]},{"metadata":{"id":"HdfGGsEbE9qa","colab_type":"code","colab":{}},"cell_type":"code","source":["@interact(i=(0, 10))\n","def show_pca_digits(i=1):\n","    plt.figure(figsize=(4,4))\n","    actual_sample = X[i].reshape(28,28)\n","    reconst_sample = (reconst[i, :] * std + mu).reshape(28, 28)\n","    plt.imshow(np.hstack([actual_sample, reconst_sample]), cmap='gray')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3KURJcLLE9qh","colab_type":"text"},"cell_type":"markdown","source":["## 2. PCA for high-dimensional datasets\n","\n","Sometimes, the dimensionality of our dataset may be larger than the number of samples we\n","have. Then it might be inefficient to perform PCA with your implementation above. Instead,\n","as mentioned in the lectures, you can implement PCA in a more efficient manner, which we\n","call PCA for high dimensional data (PCA_high_dim).\n","\n","Below are the steps for performing PCA for high dimensional dataset\n","1. Compute the matrix $XX^T$ (a $N$ by $N$ matrix with $N << D$)\n","2. Compute eigenvalues $\\lambda$s and eigenvectors $V$ for $XX^T$\n","3. Compute the eigenvectors for the original covariance matrix as $X^TV$. Choose the eigenvectors associated with the M largest eigenvalues to be the basis of the principal subspace $U$.\n","4. Compute the orthogonal projection of the data onto the subspace spanned by columns of $U$."]},{"metadata":{"id":"oCwgzUR9E9qj","colab_type":"code","colab":{}},"cell_type":"code","source":["# GRADED FUNCTION: DO NOT EDIT\n","### PCA for high dimensional datasets\n","\n","def PCA_high_dim(X, n_components):\n","    \"\"\"Compute PCA for small sample size but high-dimensional features. \n","    Args:\n","        X: ndarray of size (N, D), where D is the dimension of the sample,\n","           and N is the number of samples\n","        num_components: the number of principal components to use.\n","    Returns:\n","        X_reconstruct: (N, D) ndarray. the reconstruction\n","        of X from the first `num_components` pricipal components.\n","    \"\"\"\n","    return X # <-- EDIT THIS"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tpecg-gqE9qp","colab_type":"text"},"cell_type":"markdown","source":["Given the same dataset, `PCA_high_dim` and `PCA` should give the same output. \n","Assuming we have implemented `PCA`, correctly, we can then use `PCA` to test the correctness\n","of `PCA_high_dim`.\n","\n","We can use this __invariant__\n","to test our implementation of PCA_high_dim, assuming that we have correctly implemented `PCA`."]},{"metadata":{"id":"SejN1IL1E9qq","colab_type":"code","colab":{}},"cell_type":"code","source":["np.testing.assert_almost_equal(PCA(Xbar, 2), PCA_high_dim(Xbar, 2))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2FpPywXyE9qv","colab_type":"text"},"cell_type":"markdown","source":["Now let's compare the running time between `PCA` and `PCA_high_dim`.\n","\n","__Tips__ for running benchmarks or computationally expensive code:\n","\n","When you have some computation that takes up a non-negligible amount of time. Try separating\n","the code that produces output from the code that analyzes the result (e.g. plot the results, comput statistics of the results). In this way, you don't have to recompute when you want to produce more analysis."]},{"metadata":{"id":"-yTCPytTE9qy","colab_type":"code","colab":{}},"cell_type":"code","source":["def time(f, repeat=100):\n","    times = []\n","    for _ in range(repeat):\n","        start = timeit.default_timer()\n","        f()\n","        stop = timeit.default_timer()\n","        times.append(stop-start)\n","    return np.mean(times), np.std(times)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ASuIEayLE9q4","colab_type":"code","colab":{}},"cell_type":"code","source":["times_mm0 = []\n","times_mm1 = []\n","\n","for datasetsize in np.arange(4, 784, step=20):\n","    XX = Xbar[:datasetsize]\n","    mu, sigma = time(lambda : XX.T @ XX)\n","    times_mm0.append((datasetsize, mu, sigma))\n","    \n","    mu, sigma = time(lambda : XX @ XX.T)\n","    times_mm1.append((datasetsize, mu, sigma))\n","    \n","times_mm0 = np.asarray(times_mm0)\n","times_mm1 = np.asarray(times_mm1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"actSRwRTE9q-","colab_type":"code","colab":{}},"cell_type":"code","source":["fig, ax = plt.subplots()\n","ax.set(xlabel='size of dataset', ylabel='running time')\n","bar = ax.errorbar(times_mm0[:, 0], times_mm0[:, 1], times_mm0[:, 2], label=\"$X^T X$ (PCA)\", linewidth=2)\n","ax.errorbar(times_mm1[:, 0], times_mm1[:, 1], times_mm1[:, 2], label=\"$X X^T$ (PCA_high_dim)\", linewidth=2)\n","ax.legend();"],"execution_count":0,"outputs":[]},{"metadata":{"id":"re7H5ufwE9rC","colab_type":"text"},"cell_type":"markdown","source":["We first benchmark the time taken to compute $\\boldsymbol X^T\\boldsymbol X$ and $\\boldsymbol X\\boldsymbol X^T$. Jupyter's magic command `%time` is quite handy."]},{"metadata":{"id":"kMEbU108E9rC","colab_type":"text"},"cell_type":"markdown","source":["Next we benchmark PCA, PCA_high_dim."]},{"metadata":{"id":"qMfzPxUfE9rD","colab_type":"code","colab":{}},"cell_type":"code","source":["times0 = []\n","times1 = []\n","\n","for datasetsize in np.arange(4, 784, step=100):\n","    XX = Xbar[:datasetsize]\n","    npc = 2\n","    mu, sigma = time(lambda : PCA(XX, npc), repeat=10)\n","    times0.append((datasetsize, mu, sigma))\n","    \n","    mu, sigma = time(lambda : PCA_high_dim(XX, npc), repeat=10)\n","    times1.append((datasetsize, mu, sigma))\n","    \n","times0 = np.asarray(times0)\n","times1 = np.asarray(times1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K711HLxXE9rG","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, use the `time` magic command."]},{"metadata":{"id":"vuiSl6c1E9rG","colab_type":"code","colab":{}},"cell_type":"code","source":["%time Xbar.T @ Xbar\n","%time Xbar @ Xbar.T\n","pass # Put this here so that our output does not show result of computing `Xbar @ Xbar.T`"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wTfys0rnE9rJ","colab_type":"text"},"cell_type":"markdown","source":["We can also compare the running time for __PCA__ and __PCA\\_high\\_dim__ directly. Spend some time and think about what this plot means. We mentioned in lectures that PCA_high_dim are advantageous when\n","we have dataset size $N$ < data dimension $M$. Although our plot does not for the two running time does not intersect exactly at $N = M$, it does show the trend."]},{"metadata":{"id":"NUkukpZwE9rK","colab_type":"code","colab":{}},"cell_type":"code","source":["fig, ax = plt.subplots()\n","ax.set(xlabel='number of datapoints', ylabel='run time')\n","ax.errorbar(times0[:, 0], times0[:, 1], times0[:, 2], label=\"PCA\", linewidth=2)\n","ax.errorbar(times1[:, 0], times1[:, 1], times1[:, 2], label=\"PCA_high_dim\", linewidth=2)\n","ax.legend();"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m0QTVxQbE9rO","colab_type":"text"},"cell_type":"markdown","source":["Again, with the magic command `time`."]},{"metadata":{"id":"bJfPdS3wE9rP","colab_type":"code","colab":{}},"cell_type":"code","source":["%time PCA(Xbar, 2)\n","%time PCA_high_dim(Xbar, 2)\n","pass"],"execution_count":0,"outputs":[]}]}