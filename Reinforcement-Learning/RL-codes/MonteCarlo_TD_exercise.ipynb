{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Dependencies. Make sure all of them are downloaded before the tutoria!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/dennybritz/reinforcement-learning\n",
    "!pip install gym \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the followings lines to check if installation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWnjtVG93qMN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('./reinforcement-learning')\n",
    "\n",
    "from lib import plotting\n",
    "from collections import defaultdict\n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from lib.envs.windy_gridworld import WindyGridworldEnv\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUOCCpuzgH0o"
   },
   "source": [
    "# Monte Carlo and Temporal Difference Learning\n",
    "\n",
    "---\n",
    "\n",
    "### Why can't we always rely on Policy / Value Iteration ?\n",
    "\n",
    "At each state, we look ahead one step at each possible action and next state. \n",
    "\n",
    "1.  We can only do this because we have a perfect model of the environment. (the transition matrix $P$). In most applications, this assumption does not hold. We would like an algorithm that can learn from $\\mathcal{experience}$, i.e. by simply interacting with the environment\n",
    "2.  DP methods are polynomial in the number of states. Many real world applications have a very large state space, making DP methods unusable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NTWHqOXgwZr"
   },
   "source": [
    "## Monte-Carlo methods\n",
    "\n",
    "\n",
    "1.   Monte Carlo (MC) methods can learn directly from experience collected by interacting with the environment. An episode of experience is a series of (State, Action, Reward, Next State) tuples.\n",
    "2.   MC methods work based on episodes. We sample episodes of experience and make updates to our estimates at the end of each episode. MC methods have high variance (due to lots of random decisions within an episode) but are unbiased.\n",
    "3. MC Policy Evaluation: Given a policy, we want to estimate the state-value function V(s). Sample episodes of experience and estimate V(s) to be the reward received from that state onwards averaged across all of your experience. The same technique works for the action-value function Q(s, a). Given enough samples, this is proven to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvKvXE53lzIA"
   },
   "source": [
    "## Prediction\n",
    "Given a policy $\\pi$,  can we find the corresponding value function $v_{\\pi}$ ? \n",
    "\n",
    "Sample episodes of experience and estimate V(s) to be the reward received from that state onwards averaged across all of your experience. The same technique works for the action-value function Q(s, a). Given enough samples, this is proven to converge.\n",
    "\n",
    "### MC prediction Algorithm\n",
    "**Input**: <br>\n",
    "a policy $\\pi$ to be evaluated <br>\n",
    "the number of episodes for which the run the algorithm, *num_episodes*\n",
    "\n",
    "**Initialize**:\n",
    "\n",
    "$ \\quad V(s) \\in \\mathbb{R}$, arbitrarily, for all $s \\in S $ <br>\n",
    "$ \\quad \\textit{VisitCount(s)}$, an empty list, for all $s \\in S$\n",
    "\n",
    "**Loop** (for *num_episodes*): <br>\n",
    "$ \\quad $ Generate an episode following $\\pi: S_0,A_0,R_1,S_1,A_1,R_2, ..., S_{T-1},A_{T-1},R_T$ <br>\n",
    "$ \\quad G \\leftarrow 0$ <br>\n",
    "$ \\quad $ **Loop** for each step of episode, $t = T-1, T-2, ..., 0$: <br>\n",
    "$ \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} $ <br>\n",
    "$ \\quad \\quad \\textit{VisitCount}(S_t) = \\textit{VisitCount}(S_t) + 1$ <br>\n",
    "$ \\quad \\quad V(S_t) = V(S_t) + \\frac{1}{\\textit{VisitCount}(S_t)} (G_t - V(S_t))$\n",
    "\n",
    "return $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZM2-WzK1gvp"
   },
   "source": [
    "### Implementation\n",
    "We consider the game of blackjack for this exercice, available [here](https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Blackjack%20Playground.ipynb). I recommend exploring the previous link, to get familiar with the API of the environment. The following code was taken from the same repository (https://github.com/dennybritz/reinforcement-learning/). If you are looking for additional material, I would start there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQMJQBO_4NA8"
   },
   "source": [
    "#### The BlackJack Environment\n",
    "This environment actually builds off the gym interface, so you can interact with it using the same function calls. Here's a quick example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hWbFVhAHOZVN",
    "outputId": "d0ff68f2-d1aa-485b-d246-ac264bd20eec"
   },
   "outputs": [],
   "source": [
    "# create env\n",
    "env = BlackjackEnv()\n",
    "\n",
    "# let's see what's hidden inside this Object\n",
    "print(vars(env))\n",
    "\n",
    "# how big is the action space ? \n",
    "print (env.action_space.n)\n",
    "\n",
    "# let's sample a random action\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# let's simulate one action\n",
    "observation, reward, done, _ = env.step(random_action)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(done)\n",
    "\n",
    "# observation : the new state after executing action \n",
    "# reward      : observed reward after executing action \n",
    "# done        : `True` if the episode is over, `False` otherwise\n",
    "\n",
    "# let's reset the environment (do this when `done` == True)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wakXebbg8QPs"
   },
   "source": [
    "Since we want to do policy evaluation, we'll need a policy. Let's create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PPCNhJKl7KZO"
   },
   "outputs": [],
   "source": [
    "def sample_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is >= 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    return 0 if score >= 20 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1: complete the following code for MC prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0, max_steps_per_episode=100):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum   = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \"\"\" IMPLEMENT THIS \"\"\"\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2321
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_BzGgGsq7NYe",
    "outputId": "8c978ca0-8e64-44bd-8844-0ab70b4c5232"
   },
   "outputs": [],
   "source": [
    "env = BlackjackEnv()\n",
    "V_10k = mc_prediction(sample_policy, env, num_episodes=10000)\n",
    "plotting.plot_value_function(V_10k, title=\"10,000 Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The expected plots should look like this \n",
    "![solution](https://i.imgur.com/Bgfjb5E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your algorithm seems to run correctly, run it longer to see what the real value prediction looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V_500k = mc_prediction(sample_policy, env, num_episodes=500000)\n",
    "plotting.plot_value_function(V_500k, title=\"500,000 Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The expected plots should look like this \n",
    "![solution](https://i.imgur.com/B7efn5Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTVeDmxE_PEy"
   },
   "source": [
    "As expected, the more episode you run, the better your estimate gets. This is shown by the plots being much smoother for $t=500000$ vs $t=10000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJjx2HECy8mr"
   },
   "source": [
    "## Control \n",
    "The idea is the same as for Dynamic Programming. Use MC Policy Evaluation to evaluate the current policy then improve the policy greedily. The Problem: How do we ensure that we explore all states if we don't know the full environment?\n",
    "\n",
    "Solution to exploration problem: Use epsilon-greedy policies instead of full greedy policies. When making a decision act randomly with probability epsilon. This will learn the optimal epsilon-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Control Algorithm\n",
    "**Input**: <br>\n",
    "small $\\epsilon > 0$  <br>\n",
    "the number of episodes for which the run the algorithm, *num_episodes*\n",
    "\n",
    "** Initialize: ** <br>\n",
    "$ \\quad \\pi \\leftarrow $ an arbitrary $\\epsilon$-soft policy <br>\n",
    "$ \\quad Q(s,a) \\in \\mathbb{R}$ (arbitrarily), for all $s \\in S, a \\in A(s)$ <br>\n",
    "$ \\quad \\textit{VisitCount(s,a)}$, an empty list, for all $s \\in S, a \\in A(s)$\n",
    "\n",
    "**Loop** (for *num_episodes*): <br>\n",
    "$ \\quad $ Generate an episode following $\\pi: S_0,A_0,R_1,S_1,A_1,R_2, ..., S_{T-1},A_{T-1},R_T$ <br>\n",
    "$ \\quad G \\leftarrow 0$ <br>\n",
    "$ \\quad $ **Loop** for each step of episode, $t = T-1, T-2, ..., 0$: <br>\n",
    "$ \\quad \\quad \\textit{VisitCount}(S_t, A_t) = \\textit{VisitCount}(S_t, A_t) + 1$ <br>\n",
    "$ \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} $ <br>\n",
    "$ \\quad \\quad Q(S_t, A_t) = Q(S_t, A_t) + \\frac{1}{\\textit{VisitCount}(S_t, A_t)} (G_t - Q(S_t, A_t))$\n",
    "\n",
    "$\\quad $** Update ** $\\pi$ to be $\\epsilon$-greedy w.r.t. the update $Q$ values\n",
    "\n",
    "return $Q, \\ \\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note: \n",
    "Since we are doing control, we will want to estimate the value of (state, action) pairs. We will therefore be working with $Q's$ and not $V's$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nku_kyo0-XN-"
   },
   "source": [
    "### Implementation\n",
    "It's important to break ties arbitrarily when doing control. This is especially important when you initialize all $Q$ or $V$ array to all 0s. If you don't break ties arbitrarily, you will end up always choosing the same action!. Here is a ** argmax ** function that break ties randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def argmax(numpy_array):\n",
    "    \"\"\" argmax implementation that chooses randomly between ties \"\"\"\n",
    "    max_indices = np.where(numpy_array == numpy_array.max())[0]\n",
    "    return max_indices[np.random.randint(max_indices.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also providing you with the following function: Given a $Q$ dictionnary and $\\epsilon$, it returns a $\\epsilon$-greedy policy. Also, since the argument $Q$ is a python object, the returned $\\epsilon$-greedy policy will automatically update as you change the $Q$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B4fPdsKA-g72"
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2: complete the following code for MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ytvWvkru-lZh"
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1, max_steps_per_episode=100):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum   = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \"\"\" IMPLEMENT HERE \"\"\"\n",
    "\n",
    "        for t in range(max_steps_per_episode):\n",
    "            \"\"\" IMPLEMENT HERE \"\"\"\n",
    "            \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m4egtAFA-z22",
    "outputId": "866e90cf-2ca2-4c43-8b9a-242f6a38ac56"
   },
   "outputs": [],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1152
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2swlQVnm-rYP",
    "outputId": "885a0bb8-ef13-4b40-f4f6-856acd8c920a"
   },
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The expected plots should look like this \n",
    "![solution](https://i.imgur.com/eQkuPYx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3By1F_amH4v"
   },
   "source": [
    "### Comments on the BlackJack Env\n",
    "\n",
    "Although we have complete knowledge of the environment in the blackjack task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events—in particular, they require the environments dynamics as given by the four-argument function p—and it is not easy to determine this for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick. What is his probability of terminating with a reward of +1 as a function of the dealer’s showing card? All of the probabilities must be computed before DP can be applied, and such computations are often complex and error-prone. In contrast, generating the sample games required by Monte Carlo methods is easy. This is the case surprisingly often; the ability of Monte Carlo methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment’s dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9ZTpdSw_T4d"
   },
   "source": [
    "### Comments on MC methods\n",
    "So far we have been using the following update rule for MC Control and Prediction\n",
    "\n",
    "1) For a given state action pair $(s_t, a_t)$, observe the return $G_t$\n",
    "\n",
    "2) Update your estimate in this fashion $V(s_t)$ = $V(s_t) + \\frac{1}{VisitCount(s_t)} \\bigg[G_t - V(s_t)\\bigg] $\n",
    "\n",
    "It's important to notice that this assings **equal importance to all returns**. In general, this can be problematic when working with nonstationary environments. In this setup, it would be better to *trust/rely more* on newer $Returns$, and rely less on older ones. One easy way to achieve this is by using the following update rule\n",
    "\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\bigg[ G_t - V(s_t) \\bigg]$$\n",
    "\n",
    "This update rule will assign exponentially decresing weights over time. Feel free to (re) implement the above exercises with this update rule. You will not need the returns_count array anymore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l0gHJFaVBHK"
   },
   "source": [
    "### Drawbacks of Monte Carlo methods\n",
    "\n",
    "\n",
    "1.   You need to wait until the end of the episode to perform an update. This can be probablematic if the episode lengths are very long, (or even infinite!)\n",
    "2.   MC methods, while unbiased, tend to have high variance. To see this, note that $$G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T - t - 1}R_{T}$$ What will be the variance of $G_t$ ? Assuming all the returns are independent*, $$Var(G_t) = Var(R_{t+1}) + \\gamma^2 Var(R_{t+2}) + ... + \\gamma^{2(T - t - 1)}Var(R_{T})$$\n",
    "\n",
    "which can become quite large for long episodes.\n",
    "\n",
    "Ideally, we would like a way to trade off bias for variance. This leads us to the next topic of Temporal Difference Learning. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Independence of returns is not true in general, but the intuition of the variance growing with the size of the episode is still valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvGnBu8MANzd"
   },
   "source": [
    "## Temporal Difference (TD) learning\n",
    "TD-Learning is a combination of Monte Carlo and Dynamic Programming ideas. Like Monte Carlo, TD works based on samples and doesn't require a model of the environment. Like Dynamic Programming, TD uses bootstrapping to make updates. At the core of TD learning is the following update rule \n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\bigg[ R_{t+1} + \\gamma V(S_{t+1}) - V(s_t) \\bigg]$$\n",
    "\n",
    "(to contrast with MC, which updates like so )\n",
    "$$V(s_t) \\leftarrow V(s_t) + \\alpha \\bigg[ G_t - V(s_t) \\bigg]$$\n",
    "\n",
    "In other words, for every update, the target is actually based on our predicted value for the next state, $ R_{t+1} + \\gamma V(S_{t+1})$, instead of being the sum of observed rewards, $G_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3a2JOhR-esm6"
   },
   "source": [
    "Why is this valid ? simply because\n",
    "\\begin{align*}\n",
    "  v_{\\pi}(s) &= \\mathbb{E}[G_t | S_t = s] \\\\\n",
    "                    &= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n",
    "                    &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s]\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the pseudo code for doing policy evaluation with TD. We will not implement it in this tutorial, but feel free to go back on the previous exercice and modify them from MC to TD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD  prediction Algorithm\n",
    "**Input**: <br>\n",
    "a policy $\\pi$ to be evaluated <br>\n",
    "step size $\\alpha \\in (0, 1]$ <br>\n",
    "the number of episodes for which the run the algorithm, *num_episodes*\n",
    "\n",
    "**Initialize**:\n",
    "\n",
    "$ \\quad V(s) \\in \\mathbb{R}$, arbitrarily, for all $s \\in S $, except that $V(terminal)=0$ <br>\n",
    "\n",
    "**Loop** (for *num_episodes*): <br>\n",
    "$ \\quad $ Initialize $S$ <br>\n",
    "$ \\quad $ **Loop** for each step of the episode: <br>\n",
    "$ \\quad \\quad A \\leftarrow $ action given by $\\pi$ for S <br>\n",
    "$ \\quad \\quad $ Take action $A$, observe $R, S'$ <br>\n",
    "$ \\quad \\quad V(S) \\leftarrow V(S) + \\alpha \\big[ R + \\gamma V(S') - V(s) \\big]$ <br>\n",
    "return $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8mgaNuPhPJk"
   },
   "source": [
    "### TD Control and Q-Learning\n",
    "Let's skip prediction for TD methods and go straight to control. Arguably the most famous TD algorithm is Q-Learning, with the following pseudocode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7Z624d-ihGn"
   },
   "source": [
    "### Q-learning Algorithm\n",
    "**Input**: <br>\n",
    "step size $\\alpha \\in (0, 1]$ <br>\n",
    "the number of episodes for which the run the algorithm, *num_episodes*\n",
    "\n",
    "**Initialize**:\n",
    "\n",
    "$\\quad Q(s, a) \\in \\mathbb{R}$ arbitrarily, for all $s \\in S, a \\in A(s)$, except that $Q(terminal, \\cdot)=0$ <br>\n",
    "\n",
    "**Loop** (for *num_episodes*): <br>\n",
    "$ \\quad $ Initialize $S$ <br>\n",
    "$ \\quad $ **Loop** for each step of the episode: <br>\n",
    "$ \\quad \\quad $ Choose $A$ from $S$ using policy derived from $Q$, (e.g. $\\epsilon$-greedy) <br>\n",
    "$ \\quad \\quad $ Take action $A$, observe $R, S'$ <br>\n",
    "$ \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha \\big[ R + \\gamma \\max_a Q(S', a) - Q(s,a) \\big]$ <br>\n",
    "$ \\quad $ until S is terminal \n",
    "\n",
    "return $Q, \\ \\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLBnJCBkjXP7"
   },
   "source": [
    "### Implementation\n",
    "We consider the CliffWorld for this exercice, available [here](https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py). The API is the same as the other environment, i.e. you can use the same function calls to interact with it. \n",
    "![env](https://i.imgur.com/qz5Kdyf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3: complete the following code for Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "R2LO5257kEdr"
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, epsilon=0.05, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))  \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode +1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # reset environment at the beginning of every episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            \"\"\" IMPLEMENT HERE \"\"\"\n",
    "          \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "               \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1095
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bTfJitmVkFe7",
    "outputId": "597b2c81-0ce8-4c09-b0da-f15f507c61ca"
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()\n",
    "Q, stats_q_learning = q_learning(env, num_episodes=500)\n",
    "plotting.plot_episode_stats(stats_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_values(Q, state_shape=((4, 12))):\n",
    "    \"\"\" helper method to plot a heat map of the states \"\"\"\n",
    "    \n",
    "    values = np.zeros((4 * 12))\n",
    "    max_a  = [0 for _ in range(values.shape[0])]\n",
    "    for key, value in Q.items():\n",
    "        values[key] = max(value)\n",
    "        max_a[key] = int(argmax(value))\n",
    "        \n",
    "    def optimal_move(i, j):\n",
    "        left, right, down, up  = (i, max(j-1, 0)), (i, min(11, j+1)), (min(3, i+1), j), (max(0, i-1), j)\n",
    "        arr = np.array([values[up], values[right], values[down], values[left]])\n",
    "        if i == 2   and j != 11: arr[2] = -9999\n",
    "        if i == 0:  arr[0] = -999\n",
    "        if j == 0:  arr[3] = -999\n",
    "        if j == 11: arr[1] = -999\n",
    "        return argmax(arr)\n",
    "    \n",
    "    # reshape the state-value function\n",
    "    values = np.reshape(values, state_shape)\n",
    "    # plot the state-value function\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(values)\n",
    "    arrows = ['^', '>', 'v', '<']\n",
    "    index = 0\n",
    "    for (j, i), label in np.ndenumerate(values):\n",
    "        ax.text(i, j, np.round(label, 3), ha='center', va='center', fontsize=12)\n",
    "        if j != 3 or i==0:\n",
    "            ax.text(i, j + 0.4 , arrows[optimal_move(j, i)], ha='center', va='center', fontsize=12, color='red')\n",
    "        index += 1\n",
    "    plt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "    plt.title('State-Value Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_values(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the heatmap, try to follow a greedy policy. Does the trajectory align with optimal path in the previous picture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "Q-learning is an offline method, since the target update does not depend on the behavior policy (because of the max operator). The online version of Q-Learning is known as SARSA (which stands for State, Action, Reward, State, Action). Notice that in the following pseudocode, the action selected in the target update is the same as the action used in the next timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Algorithm\n",
    "**Input**: <br>\n",
    "step size $\\alpha \\in (0, 1]$ <br>\n",
    "the number of episodes for which the run the algorithm, *num_episodes*\n",
    "\n",
    "**Initialize**:\n",
    "\n",
    "$\\quad Q(s, a) \\in \\mathbb{R}$ arbitrarily, for all $s \\in S, a \\in A(s)$, except that $Q(terminal, \\cdot)=0$ <br>\n",
    "\n",
    "**Loop** (for *num_episodes*): <br>\n",
    "$ \\quad $ Initialize $S$ <br>\n",
    "$ \\quad $ Choose $A$ from $S$ using policy derived from $Q$, (e.g. $\\epsilon$-greedy) <br>\n",
    "$ \\quad $ **Loop** for each step of the episode: <br>\n",
    "$ \\quad \\quad $ Take action $A$, observe $R, S'$ <br>\n",
    "$ \\quad \\quad $ Choose $A'$ from $S'$ using policy derived from $Q$, (e.g. $\\epsilon$-greedy) <br>\n",
    "$ \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha \\big[ R + \\gamma Q(S', A') - Q(s,a) \\big]$ <br>\n",
    "$ \\quad $ until S is terminal \n",
    "\n",
    "return $Q, \\ \\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4: complete the following code for SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SARSA(env, num_episodes, discount_factor=1.0, epsilon=0.1, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))  \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode +1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        \"\"\" IMPLEMENT HERE \"\"\"\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            \"\"\" IMPLEMENT HERE \"\"\"\n",
    "    \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q, stats_q_learning = SARSA(env, num_episodes=500)\n",
    "plotting.plot_episode_stats(stats_q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As seen in the slides, you should expect the performance of SARSA to be better than Q-Learning during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_values(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5 : \n",
    "how will SARSA and Q-learning compare if you evaluate the learned policies with $\\epsilon$=0 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MonteCarlo_TD.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
