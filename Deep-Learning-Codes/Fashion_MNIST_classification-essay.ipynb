{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST classification using PyTorch\n",
    "\n",
    "In this notebook we will try to classify the Fashion MNIST dataset\n",
    "(https://github.com/zalandoresearch/fashion-mnist) using VGG-like architectures (https://arxiv.org/abs/1409.1556). This notebook is inspired from the MNIST example from PyTorch (https://github.com/pytorch/examples/tree/master/mnist), and introduce tricks to automatically tune and schedule the learning rate for SGD (see this paper, https://arxiv.org/abs/1506.01186, and FastAI course for example http://fastai.org).\n",
    "\n",
    "## Fashion MNIST\n",
    "\n",
    "This 10 class dataset is a drop-in replacement for MNIST with clothes instead of digits. MNI is arguably overused in the ML community nowadays. It is subtancially harder to classify.\n",
    "\n",
    "![fashion_mnist](fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a few functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.cm import get_cmap\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some system/model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_labels = [\"T-shirt/top\",\n",
    "                  \"Trouser\",\n",
    "                  \"Pullover\",\n",
    "                  \"Dress\",\n",
    "                  \"Coat\",\n",
    "                  \"Sandal\",\n",
    "                  \"Shirt\",\n",
    "                  \"Sneaker\",\n",
    "                  \"Bag\",\n",
    "                  \"Ankle boot\"]\n",
    "\n",
    "train_data = datasets.FashionMNIST('data', train=True, download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                   ]))\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's compute the average mean and std of the train images. We will\n",
    "use them for normalizing data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_seen = 0.\n",
    "mean = 0\n",
    "std = 0\n",
    "for train_batch, train_target in train_loader:\n",
    "    batch_size = train_batch.shape[0]\n",
    "    train_batch = train_batch.view(batch_size, -1)\n",
    "    this_mean = torch.mean(train_batch, dim=1)\n",
    "    this_std = torch.sqrt(\n",
    "        torch.mean((train_batch - this_mean[:, None]) ** 2, dim=1))\n",
    "    mean += torch.sum(this_mean, dim=0)\n",
    "    std += torch.sum(this_std, dim=0)\n",
    "    n_samples_seen += batch_size\n",
    "\n",
    "mean /= n_samples_seen\n",
    "std /= n_samples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860) tensor(0.3202)\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reload the data with a further `Normalize` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST('data', train=True, download=False,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean.view(1, -1),\n",
    "                                                            std=std.view(1, -1))]))\n",
    "\n",
    "test_data = datasets.FashionMNIST('data', train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean.view(1, -1),\n",
    "                                                           std=std.view(1, -1))]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32,\n",
    "                                          shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a very simple model, suitable for CPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=(3, 3), padding=1)\n",
    "        self.dropout_2d = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_2d(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = self.dropout_2d(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = x.view(-1, 7 * 7 * 20)  # flatten / reshape\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 28, 28]             100\n",
      "         Dropout2d-2           [-1, 10, 14, 14]               0\n",
      "            Conv2d-3           [-1, 20, 14, 14]           1,820\n",
      "         Dropout2d-4             [-1, 20, 7, 7]               0\n",
      "            Linear-5                  [-1, 128]         125,568\n",
      "           Dropout-6                  [-1, 128]               0\n",
      "            Linear-7                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 128,778\n",
      "Trainable params: 128,778\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.49\n",
      "Estimated Total Size (MB): 0.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "- Define a VGG-like model: add more convolutional and max pooling layers to increase the number of channels progressively while decreasing the dimensions of the feature maps with max pooling.\n",
    "- Try to use Adam instead of SGD in conjunction with the `find_lr` heuristic and the cosine learning rate schedule below;\n",
    "- (optional) Try data augmentation (horizontal flips, random crops, cutout...);\n",
    "- (optional) Implement the [mixup stochastic label interpolation](https://arxiv.org/abs/1710.09412);\n",
    "- (optional) Try to use batch-normalization;\n",
    "- (optional) Implement skip-connections.\n",
    "\n",
    "See how you compare to other approaches:\n",
    "- https://github.com/zalandoresearch/fashion-mnist\n",
    "- https://www.kaggle.com/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/vgg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=tensor([[0.2860]]), std=tensor([[0.3202]]))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "img, target = train_data[0]\n",
    "# n_channel, width, height\n",
    "print(img.shape)\n",
    "\n",
    "# First dimension should contain batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(img[0].numpy(), cmap=get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension of the input data should contain the batch size (due to `torch.nn` API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9) Ankle boot\n",
      "tensor([[-2.2545, -2.3153, -2.2598, -2.2710, -2.1887, -2.3496, -2.6492, -2.3440,\n",
      "         -2.2405, -2.2235]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = model(img[None, :])\n",
    "print(target, english_labels[target])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "img, target = train_data[1]\n",
    "# n_channel, width, height\n",
    "print(img.shape)\n",
    "\n",
    "# First dimension should contain batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(img[0].numpy(), cmap=get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) T-shirt/top\n",
      "tensor([[-2.4647, -2.6004, -1.9033, -2.1354, -2.3498, -2.1697, -2.5578, -2.3502,\n",
      "         -2.3116, -2.3870]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = model(img[None, :])\n",
    "print(target, english_labels[target])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-shirt/top'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_labels[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pullover'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_labels[pred.data.max(dim=1)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3599, -2.6062, -2.1216, -2.1912, -2.0018, -2.3040, -2.6661, -2.3348,\n",
       "         -2.3804, -2.2414]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9d26cd27f0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "pred.data.max(dim=1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-9c41e077d124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5481\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5483\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5484\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    644\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    645\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in test_loader:\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(image[0].numpy(), cmap=get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
      "        1, 4, 6, 0, 9, 3, 8, 8])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2ddc47059756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#ax.imshow(image[0].numpy(), cmap=get_cmap('gray'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAEG5JREFUeJzt3VuMXfV1x/HfmpkzF8Y2tvElrrGxDQZBkTDt1KQlqogIKakimUgB4YfWlao6UkFqJB6KeAmqVIlekjQPVSSnWHGkBJIqIaAKNSArCURBCAMp1waI5ZDBxhfGl/F1bqsPc4wGmL328bnT9f1I1pw56+y9l8+Z3+xz5r/3/pu7C0A+PZ1uAEBnEH4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0n1tXNj/Tbggxpu5yaBVM7qlCb8nNXy2IbCb2a3SvqGpF5J/+HuD0SPH9SwbrCbG9kkgMCzvrvmx9b9tt/MeiX9u6TPSbpG0lYzu6be9QFor0Y+82+W9Ja773X3CUkPS9rSnLYAtFoj4V8t6Xdzvh+t3vcBZrbdzPaY2Z5JnWtgcwCaqZHwz/dHhY+cH+zuO9x9xN1HKhpoYHMAmqmR8I9KWjPn+0sl7W+sHQDt0kj4n5O00czWm1m/pDslPdactgC0Wt1Dfe4+ZWZ3S/qJZof6drr7q03rDEBLNTTO7+6PS3q8Sb0AaCMO7wWSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCphmbpNbN9ksYlTUuacveRZjQFoPUaCn/Vp939SBPWA6CNeNsPJNVo+F3SE2b2vJltb0ZDANqj0bf9N7r7fjNbIelJM/tfd39q7gOqvxS2S9KgLmpwcwCapaE9v7vvr349JOkRSZvnecwOdx9x95GKBhrZHIAmqjv8ZjZsZgvP35b0WUmvNKsxAK3VyNv+lZIeMbPz6/meu/93U7oC0HJ1h9/d90q6rom9AGgjhvqApAg/kBThB5Ii/EBShB9IivADSTXjrD6gI6wv/vH16emg6A1tu+ei+FD1mdOnw7pd//uFNX/x1bp6ulDs+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcb5s5u9HkNQL9k/zARj6ZJ6N24orB26aWW47Ir/fC2sTx87HtZbqWwcv8zeOxYV1ta/2NCqa8aeH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSYpwfsZJx/DLvfqZ4LP/oyGS47KlVxee8S9Laf/hlXT01Q99la8L6O1viemW8md3Uhz0/kBThB5Ii/EBShB9IivADSRF+ICnCDyRVOs5vZjslfV7SIXe/tnrfUknfl7RO0j5Jd7j70da1iVaxvkpY98mJsD75mT8M68evKr4+fuVwvO1zl5+N60+sC+vvHltYWLtoMP5/HR29OKxXlpwL6xcvPBLWj++P198Otez5vy3p1g/dd6+k3e6+UdLu6vcAPkZKw+/uT0ka+9DdWyTtqt7eJem2JvcFoMXq/cy/0t0PSFL164rmtQSgHVp+bL+ZbZe0XZIGFc9vBqB96t3zHzSzVZJU/Xqo6IHuvsPdR9x9pKKBOjcHoNnqDf9jkrZVb2+T9Ghz2gHQLqXhN7OHJD0j6SozGzWzv5b0gKRbzOxNSbdUvwfwMVL6md/dtxaUbm5yL2iFnt6wXDaO37s4Ho9+44vx+i0YDp8eKD4GQJKGFsRj6Wbx8j09xfWyZa+46kBY37t/WVg/enw4rKsv3n47cIQfkBThB5Ii/EBShB9IivADSRF+ICku3V2raCprLxm2KRluk8+U1OP1W1/xy+hTU/G6S/zmnmvC+kDhsZ2zes8WP2+n18a9XTQQX9p79PCSsN7TW/y8zszE+72x00NhfWYifk0HFsbDlJX+4v972fBqs6YmZ88PJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0nlGeePxuml8rH6snqkwWmuo3F8qbGx/EN/+ydhfWJFPNa++KX48tszQet9i+LTiceOxqfF+tH+uH5J8forffFrUult7DWLTieWpAVDxccBTF63IV73z1+sq6ePrKcpawHwsUP4gaQIP5AU4QeSIvxAUoQfSIrwA0nlGedvZJxeCs/Jt96Sy2NPxWPlZb01Mo5/4J54HH/8injdg++UTKO9NN6+B4dXDA7F4/wnDyyIV74gHouPLpNw8kw8e9TQQNybSg8bKXlA4Le3Dob19T+ve9UfwJ4fSIrwA0kRfiApwg8kRfiBpAg/kBThB5IqHec3s52SPi/pkLtfW73vfkl/I+lw9WH3ufvjrWryfWXXv4+UXRvfSn4PBufke4Pn65fpvWJ9WN9356rC2vRQyXnlv4l/BKZKZpoum2Z7Ymnxc9M/EW/bSsbK+4ZKjp8ITE/Hr/fZifj4Bk3HvZ07XXKdg5ni5S/bPBpvu0lq2fN/W9Kt89z/dXffVP3X+uADaKrS8Lv7U5LG2tALgDZq5DP/3Wb2kpntNLN43iQAXafe8H9T0uWSNkk6IOmrRQ80s+1mtsfM9kwqnr8MQPvUFX53P+ju0+4+I+lbkjYHj93h7iPuPlJRfDIFgPapK/xmNvfPy1+Q9Epz2gHQLrUM9T0k6SZJy8xsVNJXJN1kZpskuaR9kr7Uwh4BtEBp+N196zx3P1jX1qzBueRbOZ7u9a+7b82lYf3MVSvD+tjV8cehM5+Ix9J7glPPK+PxePTExfG6pxaWXGugUnKdhP7i4ys8GOuWpIsvjeehH6jEPy9jx4sPUpieKrkGQ0lvKrkuv58pOX6it3j5IyfjgyuW//F1xcX/+WW47Fwc4QckRfiBpAg/kBThB5Ii/EBShB9Iqr2X7vbGLkPdt25tYe3MlSvCZScXxEM7E8Px78GpoeLa+Lpw0dLTansm43rfqXjYyYPWJxbF654ejOtWNvo6FJ8qbWeKn/fJifg5n+iPN37s4MKwXllUfDh52WXDTx0LXnBJleF4+eWLT4b146eL13/1soPhsqMrNhbWZiq1XzKcPT+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJNVVU3SfvP2GuP57xWPGPSXj0WeXxXUPTrGUJAsu1dwzVbLsyXjsdWo4Xv7sypLTjaPVB6fUSlLvsfhHIDqGQJJ6F8RPfE9P8fYnSy5vfeZUfKpz74n42I2B5fUfU1Jm8lg8jfahmfiJi44zWNx/Jlx2f3BciF3ATPTs+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqbaO888sGdb4n32ysD71l++Fy59885LC2uDB+PdYJT69Wt4Tj8VHl8f23pJzqEvKlZLjAGYq8f/NgqH8yZJLb5f1Vna+f+nM533Fyy9dcSJc9upLDsUrvyIuL6qcLaz1WcmxE2vi8rtnF4X1FQPxD9zYxEWFtf2nLw6XHdp/qrDWM1Hygsx9bM2PBPD/CuEHkiL8QFKEH0iK8ANJEX4gKcIPJFU6zm9mayR9R9InJM1I2uHu3zCzpZK+L2mdpH2S7nD3o9G6esfPafHP9hbW39i8IexlxTWHC2uX/VG46VJnp+Jzyw+eXlBYO3I0vn781LH+sF4pOS99pmQabA/G6n3pZLjspg1vh/Xlg/F49YahI2F9OrggwH3Lfh0u+0/vFV+fXpKeOHh1WP+XK/+rsLa0N75WwLRfwInx8zjt8fP+k9PFc1C8dTae0v3pxasLa95X+/68lkdOSbrH3a+W9ElJd5nZNZLulbTb3TdK2l39HsDHRGn43f2Au79QvT0u6XVJqyVtkbSr+rBdkm5rVZMAmu+CPvOb2TpJ10t6VtJKdz8gzf6CkBTPlwWgq9QcfjNbIOmHkr7s7vFB2R9cbruZ7TGzPRMz8bXJALRPTeE3s4pmg/9dd/9R9e6DZraqWl8lad6zMNx9h7uPuPtIf088+SGA9ikNv5mZpAclve7uX5tTekzSturtbZIebX57AFrFvGRIw8w+JelpSS9rdqhPku7T7Of+H0haK+ltSbe7+1i0rkW21G+wmxvteV69S5aE9RM3XxnWj14ZD7f1bS4eSrx8aTzctXY4HoZcPRDXe1UyzXZwXu7kTDya+9rJVWH9mb3rw/qSn8aXsF7+8EuFtZlTxaemNsPM7uLzcj+9/I1w2ZfGi4fTJOndU/Epve+dKj5lV5KmpqKpy+PX7Mq7iofLnznxqI5PHa5pnu7ScX53/4WKz/puTZIBtBxH+AFJEX4gKcIPJEX4gaQIP5AU4QeSKh3nb6ZWjvMDkJ713TrhYzWN87PnB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpErDb2ZrzOynZva6mb1qZn9Xvf9+M3vHzH5V/ffnrW8XQLP01fCYKUn3uPsLZrZQ0vNm9mS19nV3/9fWtQegVUrD7+4HJB2o3h43s9clrW51YwBa64I+85vZOknXS3q2etfdZvaSme00syUFy2w3sz1mtmdS5xpqFkDz1Bx+M1sg6YeSvuzuJyR9U9LlkjZp9p3BV+dbzt13uPuIu49UNNCElgE0Q03hN7OKZoP/XXf/kSS5+0F3n3b3GUnfkrS5dW0CaLZa/tpvkh6U9Lq7f23O/avmPOwLkl5pfnsAWqWWv/bfKOkvJL1sZr+q3nefpK1mtkmSS9on6Ust6RBAS9Ty1/5fSJpvvu/Hm98OgHbhCD8gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBS5u7t25jZYUm/nXPXMklH2tbAhenW3rq1L4ne6tXM3i5z9+W1PLCt4f/Ixs32uPtIxxoIdGtv3dqXRG/16lRvvO0HkiL8QFKdDv+ODm8/0q29dWtfEr3VqyO9dfQzP4DO6fSeH0CHdCT8Znarmf3azN4ys3s70UMRM9tnZi9XZx7e0+FedprZITN7Zc59S83sSTN7s/p13mnSOtRbV8zcHMws3dHnrttmvG77234z65X0hqRbJI1Kek7SVnd/ra2NFDCzfZJG3L3jY8Jm9qeSTkr6jrtfW73vnyWNufsD1V+cS9z977ukt/slnez0zM3VCWVWzZ1ZWtJtkv5KHXzugr7uUAeet07s+TdLesvd97r7hKSHJW3pQB9dz92fkjT2obu3SNpVvb1Lsz88bVfQW1dw9wPu/kL19rik8zNLd/S5C/rqiE6Ef7Wk3835flTdNeW3S3rCzJ43s+2dbmYeK6vTpp+fPn1Fh/v5sNKZm9vpQzNLd81zV8+M183WifDPN/tPNw053OjufyDpc5Luqr69RW1qmrm5XeaZWbor1DvjdbN1IvyjktbM+f5SSfs70Me83H1/9eshSY+o+2YfPnh+ktTq10Md7ud93TRz83wzS6sLnrtumvG6E+F/TtJGM1tvZv2S7pT0WAf6+AgzG67+IUZmNizps+q+2Ycfk7StenubpEc72MsHdMvMzUUzS6vDz123zXjdkYN8qkMZ/yapV9JOd//HtjcxDzPboNm9vTQ7ien3OtmbmT0k6SbNnvV1UNJXJP1Y0g8krZX0tqTb3b3tf3gr6O0mzb51fX/m5vOfsdvc26ckPS3pZUkz1bvv0+zn6449d0FfW9WB540j/ICkOMIPSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBS/we3gMfCBF6VBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in test_loader:\n",
    "    i = 0\n",
    "    while i < 1 :\n",
    "#         plt.subplot(2, 7, i + 1)\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        output = model(image)\n",
    "        _, pred = output.data.max(dim=1)\n",
    "        plt.imshow(image[0].squeeze().numpy())\n",
    "        print(label)\n",
    "        print(english_labels[label.item()])\n",
    "        print(english_labels[pred.data.max(dim=1)[1]])\n",
    "        #ax.imshow(image[0].numpy(), cmap=get_cmap('gray'))\n",
    "        #plt.imshow(image.squeeze().numpy())\n",
    "        plt.title('real label : {} vs predicted label: {}' .format(english_labels[label.item()],english_labels[label.item()]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [10, 1, 3, 3], but got 3-dimensional input of size [1, 28, 28] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-bc4713a8c7c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'real label : {} vs predicted label: {}'\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menglish_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-58beb8e3e4c8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# flatten / reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [10, 1, 3, 3], but got 3-dimensional input of size [1, 28, 28] instead"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    plt.subplot(2, 7, i + 1)\n",
    "    image, label = train_data[i]\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    output = model(image)\n",
    "    _, pred = output.data.max(dim=1)\n",
    "    plt.title('real label : {} vs predicted label: {}' .format(english_labels[label],english_labels[pred]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        batch_size = data.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss /= len(test_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # sum up batch loss\n",
    "            _, pred = output.data.max(dim=1)\n",
    "            # get the index of the max log-probability\n",
    "            correct += torch.sum(pred == target.data.long()).item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = float(correct) / len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f},'\n",
    "              ' Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_lr` function provides a learning rate for SGD or Adam, following heuristics from https://arxiv.org/abs/1506.01186:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loop_loader(data_loader):\n",
    "    while True:\n",
    "        for elem in data_loader:\n",
    "            yield elem\n",
    "\n",
    "def find_lr(model, train_loader, init_lr, max_lr, steps, n_batch_per_step=30):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=init_lr)\n",
    "    current_lr = init_lr\n",
    "    best_lr = current_lr\n",
    "    best_loss = float('inf')\n",
    "    lr_step = (max_lr - init_lr) / steps\n",
    "\n",
    "    loader = loop_loader(train_loader)\n",
    "    for i in range(steps):\n",
    "        mean_loss = 0\n",
    "        n_seen_samples = 0\n",
    "        for j, (data, target) in enumerate(loader):\n",
    "            if j > n_batch_per_step:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            mean_loss += loss.item() * data.shape[0]\n",
    "            n_seen_samples += data.shape[0]\n",
    "            optimizer.step()\n",
    "\n",
    "        mean_loss /= n_seen_samples\n",
    "        print('Step %i, current LR: %f, loss %f' % (i, current_lr, mean_loss))\n",
    "            \n",
    "        if np.isnan(mean_loss) or mean_loss > best_loss * 4:\n",
    "            return best_lr / 4\n",
    "        \n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_lr = current_lr\n",
    "\n",
    "        current_lr += lr_step\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "    return best_lr / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our model on the GPU if required. We then define an optimizer and a learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, current LR: 0.000100, loss 2.310490\n",
      "Step 1, current LR: 0.010099, loss 2.256100\n",
      "Step 2, current LR: 0.020098, loss 1.951430\n",
      "Step 3, current LR: 0.030097, loss 1.363623\n",
      "Step 4, current LR: 0.040096, loss 1.091958\n",
      "Step 5, current LR: 0.050095, loss 0.998775\n",
      "Step 6, current LR: 0.060094, loss 0.952295\n",
      "Step 7, current LR: 0.070093, loss 0.959676\n",
      "Step 8, current LR: 0.080092, loss 0.865822\n",
      "Step 9, current LR: 0.090091, loss 0.805868\n",
      "Step 10, current LR: 0.100090, loss 0.783902\n",
      "Step 11, current LR: 0.110089, loss 0.746506\n",
      "Step 12, current LR: 0.120088, loss 0.728083\n",
      "Step 13, current LR: 0.130087, loss 0.706955\n",
      "Step 14, current LR: 0.140086, loss 0.660437\n",
      "Step 15, current LR: 0.150085, loss 0.694503\n",
      "Step 16, current LR: 0.160084, loss 0.738720\n",
      "Step 17, current LR: 0.170083, loss 0.654181\n",
      "Step 18, current LR: 0.180082, loss 0.732505\n",
      "Step 19, current LR: 0.190081, loss 0.591987\n",
      "Step 20, current LR: 0.200080, loss 0.660843\n",
      "Step 21, current LR: 0.210079, loss 0.662156\n",
      "Step 22, current LR: 0.220078, loss 0.615749\n",
      "Step 23, current LR: 0.230077, loss 0.628480\n",
      "Step 24, current LR: 0.240076, loss 0.651674\n",
      "Step 25, current LR: 0.250075, loss 0.647539\n",
      "Step 26, current LR: 0.260074, loss 0.709796\n",
      "Step 27, current LR: 0.270073, loss 0.761885\n",
      "Step 28, current LR: 0.280072, loss nan\n",
      "Best LR 0.04752025000000003\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.reset_parameters()\n",
    "lr = find_lr(model, train_loader, 1e-4, 1, 100, 30)\n",
    "model.reset_parameters()\n",
    "\n",
    "print('Best LR', lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the learning rate\n",
    "#find_lr()\n",
    "#find_lr(model, train_loader, init_lr, max_lr, steps, n_batch_per_step=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       T_max=3,\n",
    "                                                       last_epoch=-1)\n",
    "\n",
    "logs = {'epoch': [], 'train_loss': [], 'test_loss': [],\n",
    "        'test_accuracy': [], 'lr': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315573\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.960153\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.000861\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.665569\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.551405\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.864002\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.587267\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.663400\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.386711\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.589336\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.558548\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.733056\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.669724\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.756137\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.405621\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.702884\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.498881\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.465019\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.417093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lionel/miniconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4093, Accuracy: 8492/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.986231\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.482738\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.333017\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.496554\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.527661\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.697987\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.326214\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.400836\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.635921\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.212432\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.698776\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.741996\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.656644\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.272325\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.344327\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.409594\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.388584\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.514413\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.387663\n",
      "\n",
      "Test set: Average loss: 0.3601, Accuracy: 8663/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.353928\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.241889\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.681594\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.275990\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.321454\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.293371\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.485728\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.352735\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.484733\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.590718\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.410031\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.291673\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.394760\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.242572\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.527193\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.393523\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.237838\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.676855\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.398259\n",
      "\n",
      "Test set: Average loss: 0.3293, Accuracy: 8804/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.440576\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.357397\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.211258\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.443177\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.280381\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.318173\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.301564\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.322774\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.245165\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.723636\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.477380\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.562599\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.106184\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.352187\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.451133\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.414517\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.218214\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.475746\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.296501\n",
      "\n",
      "Test set: Average loss: 0.3146, Accuracy: 8855/10000 (89%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.311248\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.360393\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.158338\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.257355\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.250447\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.326438\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.366299\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.359834\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.171007\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.723618\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.816506\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.313687\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.270192\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.500984\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.203641\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.258606\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.537531\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.281337\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.412985\n",
      "\n",
      "Test set: Average loss: 0.3146, Accuracy: 8855/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.582749\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.134730\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.478481\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.276440\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.536699\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.437377\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.196442\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.456291\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.503274\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.180784\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.340353\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.445552\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.227478\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.388076\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.519434\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.414180\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.316500\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.196048\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.235290\n",
      "\n",
      "Test set: Average loss: 0.3098, Accuracy: 8860/10000 (89%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.318735\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.418027\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.257976\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.588307\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.340082\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.317866\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.375976\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.314742\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.447186\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.357519\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.232200\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.458429\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.144491\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.373734\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.400677\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.325345\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.236235\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.382045\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.591945\n",
      "\n",
      "Test set: Average loss: 0.3068, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.459258\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.305794\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.372865\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.224164\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.647646\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.273147\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.291294\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.382500\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.338249\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.809614\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.560570\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.219915\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.459723\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.727924\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.183934\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.357460\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.380819\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.359989\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.500510\n",
      "\n",
      "Test set: Average loss: 0.3129, Accuracy: 8843/10000 (88%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.365143\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.246642\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.283494\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.314151\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.434918\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.252678\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.539363\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.246325\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.277238\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.197245\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.329664\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.432654\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.383318\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.262325\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.269241\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.401549\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.218261\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.159763\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.290783\n",
      "\n",
      "Test set: Average loss: 0.2879, Accuracy: 8944/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.345348\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.220848\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.163343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.321392\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.291418\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.202398\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.120967\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.355374\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.438099\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.440300\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.256307\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.369465\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.238500\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.097775\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.297381\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.380018\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.585948\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.307175\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.387571\n",
      "\n",
      "Test set: Average loss: 0.2824, Accuracy: 8990/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.293117\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.376464\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.425643\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.171209\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.641150\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.183283\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.160359\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.442034\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.258327\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.288773\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.243555\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.359747\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.232939\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.278768\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.302257\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.252634\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.037855\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.203699\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.209732\n",
      "\n",
      "Test set: Average loss: 0.2824, Accuracy: 8990/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.348284\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.310291\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.244798\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.287443\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.288927\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.405929\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.231644\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.310937\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.300312\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.206325\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.118978\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.462747\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.167814\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.212006\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.239506\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.190860\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.263676\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 1.072333\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.304628\n",
      "\n",
      "Test set: Average loss: 0.2765, Accuracy: 8987/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, train_loader, epoch)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    logs['epoch'].append(epoch)\n",
    "    logs['train_loss'].append(train_loss)\n",
    "    logs['test_loss'].append(test_loss)\n",
    "    logs['test_accuracy'].append(test_accuracy)\n",
    "    logs['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
